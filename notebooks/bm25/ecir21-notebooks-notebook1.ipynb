{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ECIR 2021 Tutorial Notebook - Part 1.ipynb","provenance":[{"file_id":"1kWCNf3QlQ4bX5YCM9OJBaaLikoTFCd5A","timestamp":1615914442515},{"file_id":"17Pihqt_C8DFzqlomTUks-5stNzNFjrAn","timestamp":1611078807322},{"file_id":"121AtOADdFd2VVAX5hcJX0WNBNt2_QHDu","timestamp":1609952873856},{"file_id":"1o4RTKOutf_FlMyPdEPkRyutnbY26JXMf","timestamp":1571324862553}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6UxEkLc6yz6J"},"source":["# PyTerrier ECIR Tutorial Notebook - Part 1\n","\n","This is one of a series of Colab notebooks created for the [ECIR 2021](https://www.ecir2021.eu) Tutorial entitled '**IR From Bag-of-words to BERT and Beyond through Practical Experiment**'. It demonstrates the use of [PyTerrier](https://github.com/terrier-org/pyterrier) on the [CORD19 test collection](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge).\n","\n","In particular, this notebooks has the following learning outcomes:\n","  - PyTerrier installation & configuration\n","  - indexing a collection\n","  - accessing an index\n","  - using the `BatchRetrieve` transformer for searching an index\n","  - conducting an `Experiment` \n","\n","Pre-requisites:\n"," - We assume that you are confident in programming Python, including [lambda functions](https://www.w3schools.com/python/python_lambda.asp).\n"," - We will **only be supporting notebooks on the Google Colab platform**.\n","  > *Explanation*: PyTerrier uses [pytrec_eval](https://github.com/cvangysel/pytrec_eval) for evaluation, which does not yet easily install on Windows. It will work fine on Linux and macOS, but only if you have the appropriate compilers installed. Hence, we prefer Google Colab.\n","\n","Related Reading:\n"," - [Pandas documentation](https://pandas.pydata.org/docs/)\n"," - [PyTerrier documentation](https://pyterrier.readthedocs.io/en/latest/)\n"]},{"cell_type":"markdown","metadata":{"id":"7u2hD-zBzfpR"},"source":["PyTerrier is a Python framework, but uses the underlying [Terrier information retrieval toolkit](http://terrier.org) for many indexing and retrieval operations. While PyTerrier was new in 2020, Terrier is written in Java and has a long history dating back to 2001. PyTerrier makes it easy to perform IR experiments in Python, but using the mature Terrier platform for the expensive indexing and retrieval operations. \n","\n","In the following, we introduce everything you need to know about PyTerrier, and also provide appropriate links to relevant parts of the [PyTerrier documentation](https://pyterrier.readthedocs.io/en/latest/).\n"]},{"cell_type":"markdown","metadata":{"id":"iH0Ds2370V0G"},"source":["### Installation & Configuration\n","\n","PyTerrier is installed as follows. This might take a few minutes, so you can read on."]},{"cell_type":"code","metadata":{"id":"5oE5neAX0bkW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615971631411,"user_tz":-60,"elapsed":27839,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"02aff377-cd89-4055-c86f-01cc0a82375a"},"source":["!pip install python-terrier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JUN1B8RI0gPC"},"source":["The next step is to initialise PyTerrier. This is performed using PyTerrier's `init()` method. The `init()` method is needed as PyTerrier must download Terrier's jar file and start the Java virtual machine. We prevent `init()` from being called more than once by checking `started()`."]},{"cell_type":"code","metadata":{"id":"Z4qALBa90-7g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615971633689,"user_tz":-60,"elapsed":17656,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"378c8773-686c-4abc-d1b5-af80d18cfed0"},"source":["import pyterrier as pt\n","if not pt.started():\n","  pt.init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-qqjVSu-5_FX"},"source":["### Documents, Indexing and Indexes"]},{"cell_type":"markdown","metadata":{"id":"3soS1IIy5B83"},"source":["Much of PyTerrier's view of the world is wrapped up in Pandas dataframes. Let's consider some textual documents in a dataframe.\n"]},{"cell_type":"code","metadata":{"id":"gSEiEuTE5uyL","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1615971633692,"user_tz":-60,"elapsed":14933,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"8ef282f5-94df-403f-c501-6147f62d2de8"},"source":["# we need to import pandas. We commonly rename it to pd, to make commands shorter\n","import pandas as pd\n","\n","# lets not truncate output too much\n","pd.set_option('display.max_colwidth', 150)\n","\n","docs_df = pd.DataFrame([\n","        [\"d1\", \"this is the first document of many documents\"],\n","        [\"d2\", \"this is another document\"],\n","        [\"d3\", \"the topic of this document is unknown\"]\n","    ], columns=[\"docno\", \"text\"])\n","\n","docs_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2RCtCCTU6GAj"},"source":["Before any search engine can estimate which documents are most likely to be relevant for a given query, it must index the documents. \n","\n","In the following cell, we index the dataframe's documents. The index, with all its data structures, is written into a directory called `index_3docs`. "]},{"cell_type":"code","metadata":{"id":"1YvLhEOS6V8w","colab":{"base_uri":"https://localhost:8080/","height":37},"executionInfo":{"status":"ok","timestamp":1615971681350,"user_tz":-60,"elapsed":2312,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"ebe4070d-7160-42ba-bbfe-e5ccc2f3165b"},"source":["indexer = pt.DFIndexer(\"./index_3docs\", overwrite=True)\n","index_ref = indexer.index(docs_df[\"text\"], docs_df[\"docno\"])\n","index_ref.toString()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TUm6r6_625gW"},"source":["An `IndexRef`\n"," is essentially a string saying where an index is stored. Indeed, we can look in the `index_3docs` directory and see that it has created various small files: "]},{"cell_type":"code","metadata":{"id":"TF45pl5O8p7R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615971697027,"user_tz":-60,"elapsed":581,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"c039d811-aebe-4e4d-f1d4-18b2b989d4c5"},"source":["!ls -lh index_3docs/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B2b8isFP3Kv6"},"source":["With an `IndexRef`, we can load it to an actual index. The method `pt.IndexFactory.of()` is the relevant factory. "]},{"cell_type":"code","metadata":{"id":"TTM17szD6pNy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615971763026,"user_tz":-60,"elapsed":587,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"0d00514e-b30f-4b0c-a991-d618d24bb756"},"source":["index = pt.IndexFactory.of(index_ref)\n","\n","#lets see what type index is.\n","type(index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZe3HD5i7G3v"},"source":["Ok, so this object refers to Terrier's [`Index`](http://terrier.org/docs/current/javadoc/org/terrier/structures/Index.html) type. Check the linked Javadoc – you will see that this Java object has methods such as:\n"," - `getCollectionStatistics()`\n"," - `getInvertedIndex()`\n"," - `getLexicon()`\n","\n","Let's see what is returned by the `CollectionStatistics()` method:"]},{"cell_type":"code","metadata":{"id":"6-gXEDSX65bx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615972787602,"user_tz":-60,"elapsed":629,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"5491223b-5b50-48c2-8232-d75970e2c2ae"},"source":["print(index.getCollectionStatistics().toString())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i6HrR4lc7i10"},"source":["Ok, that seems fair – we have 3 documents. But why only 4 terms? \n","Let's check the [`Lexicon`](http://terrier.org/docs/current/javadoc/org/terrier/structures/Lexicon.html), which is our vocabulary. Fortunately, the `Lexicon` can be iterated easily from Python:"]},{"cell_type":"code","metadata":{"id":"us2mAzTW7Bny","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615971875714,"user_tz":-60,"elapsed":585,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"e10d37e2-84a4-448d-9ed9-3d72a5ad5ab7"},"source":["for kv in index.getLexicon():\n","  print(\"%s (%s) -> %s (%s)\" % (kv.getKey(), type(kv.getKey()), kv.getValue().toString(), type(kv.getValue()) ) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwbp94gh86pw"},"source":["Here, iterating over the `Lexicon` returns a pair of `String ` term and a [`LexiconEntry`](http://terrier.org/docs/current/javadoc/org/terrier/structures/LexiconEntry.html) object – which itself is an [`EntryStatistics`](http://terrier.org/docs/current/javadoc/org/terrier/structures/EntryStatistics.html) – and contains information including the statistics of that term.\n","\n","\n","So what did we find? Here are some observations:\n"," - we only have 4 unique terms, as stopwords were removed;\n"," - we have one term for `\"document\"`, even though `\"documents\"` occurred in document \"`d1`\". \n"," \n","Both these observations make sense, as indeed Terrier removes standard stopwords and applies Porter's stemmer by default.\n","\n","Further:\n"," - `Nt` is the number of unique documents that each term occurs in – this is useful for calculating IDF.\n"," - `TF` is the total number of occurrences – some weighting models use this instead of Nt.\n"," - The numbers in the `@{}` are a pointer – they tell Terrier where the postings are for that term in the inverted index data structure.\n","\n","Finally, we can also use the square bracket notation to lookup terms in Terrier's lexicon:\n"]},{"cell_type":"code","metadata":{"id":"SZmi9498-Ijw","colab":{"base_uri":"https://localhost:8080/","height":37},"executionInfo":{"status":"ok","timestamp":1615972070133,"user_tz":-60,"elapsed":591,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"1dccc860-2f99-4e62-b1cc-da4918003b11"},"source":["index.getLexicon()[\"document\"].toString()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vaKaU59l-kzg"},"source":["Let's now think about the inverted index. Remember that the inverted index tells us in which *documents* each term occurs in. The `LexiconEntry` is the pointer that tell us where to find the postings for that term in the inverted index."]},{"cell_type":"code","metadata":{"id":"XQki_Pds8ut2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615972108524,"user_tz":-60,"elapsed":640,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"f7824c65-33d9-499f-f9dc-97d8d7e652c5"},"source":["pointer = index.getLexicon()[\"document\"]\n","for posting in index.getInvertedIndex().getPostings(pointer):\n","    print(posting.toString() + \" doclen=%d\" % posting.getDocumentLength())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7EaoIIO_DPx"},"source":["Ok, so we can see that `\"document\"` occurs once in each of the three documents. \n","\n","NB: Terrier counts documents as integers from 0 (called *docids*). It records the mapping back to *docnos* (the string form, i.e. \"`d1`\", \"`d2`\") in a separate data structure called the *metaindex*."]},{"cell_type":"markdown","metadata":{"id":"zOSdVAr-CGRf"},"source":["### Searching an Index\n","\n","Our way into search in PyTerrier is called `BatchRetrieve`. BatchRetrieve is configured by specifying an index and a weighting model (`Tf` in our example). We then search for a single-word query, `\"document\"`."]},{"cell_type":"code","metadata":{"id":"XtK93nwXCF5C","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1615972796605,"user_tz":-60,"elapsed":1097,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"f10dccb8-7d91-44ba-8afa-b21b32022999"},"source":["br = pt.BatchRetrieve(index, wmodel=\"Tf\")\n","br.search(\"document\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHqSfTCtDM2T"},"source":["So the `search()` method returns a dataframe with columns:\n"," - `qid`: this is by default \"1\", since it's our first and only query\n"," - `docid`: Terrier' internal integer for each document\n"," - `docno`: the external (string) unique identifier for each document\n"," - `score`: since we use the `Tf` weighting model, this score corresponds the total frequency of the query (terms) in each document\n"," - `rank`: A handy attribute showing the descending order by score\n"," - `query`: the input query\n","\n","As expected, the `Tf` weighting model used here only counts the frequencies of the query terms in each document, i.e.:\n","$$\n","score(d,q) = \\sum_{t \\in q} tf_{t,d}\n","$$\n","\n","Hence, it's clear that document `d1` should be the highest scored document with two occurrences (c.f. `'document'` and `'documents'`).  "]},{"cell_type":"markdown","metadata":{"id":"BJBXquPOD6q7"},"source":["We can also pass a dataframe of one or more queries to the `transform()` method (rather than the `search()` method) of a transformer, with queries numbered \"q1\", \"q2\" etc.. "]},{"cell_type":"code","metadata":{"id":"TPBmPOETBKWk","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1615972799893,"user_tz":-60,"elapsed":566,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"69bf68d3-5e95-403d-f5f8-76bbd7cdaba9"},"source":["import pandas as pd\n","queries = pd.DataFrame([[\"q1\", \"document\"], [\"q2\", \"first document\"]], columns=[\"qid\", \"query\"])\n","br.transform(queries)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tcgDzFLBEWAI"},"source":["In fact, we are usually calling `transform()`, so it's the default method – i.e. \n","`br.transform(queries)` can be more succinctly written as `br(queries)`."]},{"cell_type":"code","metadata":{"id":"YCwxb3HhEOp_","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1615972806683,"user_tz":-60,"elapsed":655,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"b0cfe94b-c1d7-4ac8-811d-befc9771ce32"},"source":["br(queries)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldY8VV8wQ60Z"},"source":["### CORD19\n","\n","OK, having 3 documents is quite trivial, so let's move upto a slightly larger corpus of documents. We'll be using the CORD19 datasets for the remainder of this tutorial. PyTerrier has a handy `get_dataset()` API, which allows us to download the corpus and index it."]},{"cell_type":"code","metadata":{"id":"L2lJsK-vEcQx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615972928650,"user_tz":-60,"elapsed":120273,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"bcdfe9c5-7950-4974-a97d-ab3764995115"},"source":["import os\n","\n","cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n","pt_index_path = './terrier_cord19'\n","\n","if not os.path.exists(pt_index_path + \"/data.properties\"):\n","  # create the index, using the IterDictIndexer indexer \n","  indexer = pt.index.IterDictIndexer(pt_index_path)\n","\n","  # we give the dataset get_corpus_iter() directly to the indexer\n","  # while specifying the fields to index and the metadata to record\n","  index_ref = indexer.index(cord19.get_corpus_iter(), \n","                            fields=('abstract',), \n","                            meta=('docno',))\n","\n","else:\n","  # if you already have the index, use it.\n","  index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n","index = pt.IndexFactory.of(index_ref)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7GK9uANRt8w"},"source":["#### Task 1\n","- What are the statistics of our index?"]},{"cell_type":"code","metadata":{"id":"bNAVqf9uRr2p"},"source":["#YOUR SOLUTION"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tQD9Q8CqSirN"},"source":["Next, CORD19 also has a corresponding set of queries and relevance assessments (aka qrels), thus forming a *test collection*, \n","\n","We can easily access the topics and qrels from the dataset. Indeed these are expressed as dataframes as well (we use Pandas's [`head()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) method to show only the first 5 topics):"]},{"cell_type":"code","metadata":{"id":"8n7oY-YYS_-A","colab":{"base_uri":"https://localhost:8080/","height":275},"executionInfo":{"status":"ok","timestamp":1615972942774,"user_tz":-60,"elapsed":961,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"6ff6f437-725f-4b92-bfab-787e18386941"},"source":["cord19.get_topics(variant='title').head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rYxqvhJTGNX","colab":{"base_uri":"https://localhost:8080/","height":275},"executionInfo":{"status":"ok","timestamp":1615972945620,"user_tz":-60,"elapsed":1704,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"e4db9772-f10f-4133-bc25-aee1a6f5a0ba"},"source":["cord19.get_qrels().head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Gop4-jVbIIu"},"source":["### Weighting Models\n","\n","So far, we have been using the simple \"`Tf`\" as our ranking function for document retrieval in BatchRetrieve. However, we can use other models such as `\"TF_IDF\"` by simply changing the `wmodel=\"Tf\"` keyword argument in the constructor of `BatchRetrieve`.\n"]},{"cell_type":"code","metadata":{"id":"Cg8AGzCibdPG","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1615973062514,"user_tz":-60,"elapsed":945,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"bc353f41-1c33-468c-a834-37a93f1d0ced"},"source":["tfidf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n","tfidf.search(\"chemical reactions\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6aZGX9sbdmc"},"source":["You will note that, as expected, the scores of documents ranked by `TF_IDF` are no longer integers. You can see the exact formula used by Terrier from [the Github repo](https://github.com/terrier-org/terrier-core/blob/5.x/modules/core/src/main/java/org/terrier/matching/models/TF_IDF.java#L79).\n","\n","Terrier supports many weighting models – the documentation contains [a list of supported models](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html) - some of which we will discover later in the tutorial.\n"]},{"cell_type":"markdown","metadata":{"id":"YQ0j9lFfx-gO"},"source":["### What is Success?\n","\n","So far, we have been creating search engine models, but we haven't decided if any of them ia actually any good. Let's investigate if we are getting a correct (\"relevant\") document at the first rank."]},{"cell_type":"code","metadata":{"id":"iyShZYpwwNSx","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1615973109097,"user_tz":-60,"elapsed":794,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"91f1ae8b-8f3a-4547-ae37-d9d808c970ce"},"source":["qrels = cord19.get_qrels()\n","def get_res_with_labels(ranker, df):\n","  # get the results for the query or queries\n","  results = ranker( df )\n","  # left outer join with the qrels\n","  with_labels = results.merge(qrels, on=[\"qid\", \"docno\"], how=\"left\").fillna(0)\n","  return with_labels\n","\n","# lets get the Tf results for the first query\n","get_res_with_labels(tfidf, cord19.get_topics(variant='title').head(1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"id":"OFUmiFSobUDg","executionInfo":{"status":"ok","timestamp":1615973119937,"user_tz":-60,"elapsed":3826,"user":{"displayName":"Nicola Tonellotto","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gga8BaxLsPFWvzzBzSximki7T2Jsnf0EEARTd_h=s64","userId":"17533833776178224794"}},"outputId":"db01e114-e6e7-4ff4-9447-ecf5e3d8ea54"},"source":["pt.Experiment(\n","    [tfidf],\n","    cord19.get_topics(variant='title'),\n","    cord19.get_qrels(),\n","    eval_metrics=[\"map\", \"ndcg\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mt0iPhRw2J-S"},"source":["## That's all folks\n","\n","The following parts of the PyTerrier documentation may be useful references for this notebook:\n"," * [PyTerrier datasets](https://pyterrier.readthedocs.io/en/latest/datasets.html)\n"," * [Using Terrier for retrieval](https://pyterrier.readthedocs.io/en/latest/terrier-indexing.html)\n"," * [Transformers in PyTerrier](https://pyterrier.readthedocs.io/en/latest/transformer.html)\n"," * [Transformer Operators](https://pyterrier.readthedocs.io/en/latest/operators.html)"]},{"cell_type":"code","metadata":{"id":"mRjyEZ5_aTvM"},"source":[],"execution_count":null,"outputs":[]}]}
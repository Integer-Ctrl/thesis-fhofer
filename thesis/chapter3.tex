\chapter{Methodology}\label{methodology}

In this Chapter, I present the individual steps of the pipeline used to process the datasets. I begin by introducing all the datasets on which the transfer process was performed and evaluated. Subsequently, I discuss each step of the transfer pipeline in detail. The goal is to automatically infer \texttt{qrels} for the target dataset through pairwise preferences based on the existing \texttt{qrels} of the source dataset.
\\\\
The first step in the transfer pipeline is the segmentation of a selection of documents in the source dataset into passages. Relevant information within a document, if present, is typically confined to specific parts and not in the whole document. To improve the results in the pairwise preferences later, the goal is on filtering out only those passages that are highly relevant to a query.
\\\\
The second step identifies the passages of a document that are most relevant to the query of its \texttt{qrel}. For each \texttt{qrel} in the source dataset, i.e., the \texttt{(query, document, label)} triples, all passages of the document are treated as individual queries and submitted as requests against the source dataset. Based on the responses to these requests, various metrics are calculated to measure the proportion of relevant documents retrieved in the document ranking. Passages containing a large amount of relevant information for the query of the \texttt{qrel} retrieve more relevant documents and achieve better metrics than those contributing little or no information to the query.

\pagebreak

In the third step, the quality of the assigned passage scores is evaluated using different rank correlation methods. Based on the evaluation results, the metric with the highest rank correlation is used to select candidates for the pairwise preferences. In this selection phase, documents, referred to as candidates, are identified in the target dataset for which \texttt{qrels} will be determined. A candidate, similar to a \texttt{qrel}, consists of a query $q$, a candidate $c$, a set of known, relevant passages, and a set of known, non relevant passages for $q$ from the source dataset. Since the relevant and non relevant passages are later used for the pairwise preference, each candidate $c$ from the target dataset is also divided into passages $c=(c_1, c_2, ..., c_n)$.
\\\\
In the final step, the pairwise preferences are inferred. All candidates, i.e., triples consisting of \texttt{(query text, non/relevant passage text, unknown passage text)}, are fed into a pairwise ranking model to determine whether the unknown passage is as relevant as the known passage concerning the query text. From the results of the pairwise preferences, labels for the respective unknown passages are generated through the aggregation of all pairwise preferences with the known passages. A label for an entire document in the target dataset is then created by aggregating the labels of all its passages.

\pagebreak


% ========
% Datasets
% ========
\section{Datasets}\label{datasets}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{./graphics/drawio/datasets.png}
    \caption{Simplified structure of a dataset from \texttt{ir\_datasets}, comprising a set of documents, queries, and relevance assessments, along with their attributes.}
\end{figure}

\begin{table}[h!]
    \centering
    \footnotesize
    \caption{List of source datasets and their associated retrieval tasks from which existing \texttt{qrels} where transferd into the target dataset \texttt{ClueWeb22/b}.}
    \label{tab:datasets}
    \begin{tabular}{crcrrr}
        \toprule
        \multicolumn{2}{c}{\textbf{Corpus}} & \multicolumn{4}{c}{\textbf{ Associated Retrieval Tasks}} \\
        \cmidrule(lr){1-2} \cmidrule(lr){3-6}
        Name & documents  & Name & Queries &\texttt{qrels} & Labels \\
        \toprule
        
        Args.me & 0.4~m & Touché 2020 Task 1 & 49 & 2,298 & 3 \\
        \midrule

        % \multirow{4}{*}{ClueWeb09} & \multirow{4}{*}{1.0~b} & TREC 2009 Web Track & 50 & 23,601 & 3 \\
        % & & TREC 2010 Web Track & 50 & 25,329 & 4 \\
        % & & TREC 2011 Web Track & 50 & 19,381 & 4\\
        % & & TREC 2012 Web Track & 50 & 16,055 & 5 \\
        % \midrule

        % \multirow{4}{*}{ClueWeb12} & \multirow{4}{*}{731.7~m} & TREC 2013 Web Track & 50 & 14,474 & 5 \\
        % & & TREC 2014 Web Track & 50 & 14,432 & 5 \\
        % & & Touché 2021 Task 2 & 50 & 2,076 & 3 \\
        % & & Touché 2022 Task 2 & 50 & 2,107 & 3 \\
        % \midrule

        \multirow{3}{*}{Disks4+5} & \multirow{3}{*}{0.5~m} & Robust04 & 250 & 311,410 & 3 \\
        & & TREC-7 & 50 & 80,345 & 2 \\
        & & TREC-8 & 50 & 86,830 & 2 \\
        \midrule

        \multirow{2}{*}{MS MARCO} & \multirow{2}{*}{3.2~m} & TREC 2019 DL Track & 43 & 16,258 & 4 \\
        & & TREC 2020 DL Track & 45 & 9,098 & 4 \\
        
        \bottomrule
    \end{tabular}
\end{table}

To simplify data handling, I used \texttt{ir\_datasets}~\citep{macavaney:2021}, a Python package that provides numerous datasets and their associated retrieval tasks. The advantage of \texttt{ir\_datasets} is that it provides a standardized interface for accessing data. Using various iterators, the package manages access to corpora, queries, and \texttt{qrels}, enabling the transfer pipeline to handle different datasets in a uniform way. The datasets and retrieval tasks used in this research are listed in Table~\ref{tab:datasets}. The source datasets were selected because they are widely used in the field of information retrieval and because each dataset comes up with associated retrieval tasks in \texttt{ir\_datasets}. The retrieval tasks, providing \texttt{qrels} for the source datasets, are later transferd to the target dataset, \texttt{ClueWeb22/b}. The target dataset, \texttt{ClueWeb22/b}, was chosen because it is currently the newest ClueWeb corpus in the \texttt{Lemur Project}\footnote{https://lemurproject.org}. The corpus is significantly large, containing over 1.0~billion documents. Due to its size, the ratio of \texttt{qrels} to documents is relatively low. Therefore, the goal is to transfer the existing \texttt{qrels} from the source datasets and their associated retrieval tasks to generate new \texttt{qrels} for \texttt{ClueWeb22/b}, thereby enriching the corpus with additional relevance judgments. 


% =====================
% Document Segmentation
% =====================
\section{Document Segmentation}\label{document-segmentation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{./graphics/drawio/document_segmentation.png}
    \caption{Visualization of the document segmentation step in the process pipeline. First, a subset of documents is selected from the document store. Then, the selected documents are segmented using \texttt{spaCy}.}
    \label{fig:document-segmentation}
\end{figure}

Now that the source datasets, along with their retrieval tasks and the target dataset, have been selected, the actual processing can begin. The first step in the transfer pipeline involves selecting and segmenting documents from the source dataset into passages as shown in Figure~\ref{fig:document-segmentation}. The primary objective is to transfer existing relevance assessments from the source retrieval task to the target corpus. This is done by using relevant documents, given through the existing relevance assessments, for each query in the retrieval task. These documents are the basis for comparison in the later pairwise preference step, alongside with documents from the target dataset. Because not all judged documents are required, only a representative subset is selected. Once the documents are chosen, they are segmented into passages, as relevance is often confined to specific sections rather than the entire document text \textcolor{red}{REF?}. By breaking documents into passages, the pipeline can focus on the most relevant content while disregarding non-relevant parts of a document in the subsequent processing steps. Furthermore, later stages of the pipeline employ transformer models, which have a maximum input length they can process. Segmenting documents before passing them through LLMs ensures compliance with this constraint, allowing the models to work effectively \citep{levy:2024}.

% Document Selection
\subsection{Document Selection}\label{document-selection}

Instead of using the entire source corpus for relevance transfer, only a selection of documents is used, step [1] in Figure~\ref{fig:document-segmentation}. First, all documents without relevance assessments in the retrieval task can be ignored since they contain no useful information for the relevance transfer. Therefore, only judged documents are considered for selection. A document is considered judged if it has at least one relevance assessment in the \texttt{qrel store}. These judged documents, along with their relevance assessments for one or multiple queries of the retrieval task, can be used to transfer information, as their relevance has already been determined.
\\\\
As shown in Table~\ref{tab:datasets}, each retrieval task categorizes relevance assessments using different relevance labels. A relevance label specifies how relevant a document is to an information need, i.e., a query. This categorization varies in granularity. Some tasks use binary labels to distinguish only between relevant and non-relevant documents, while others employ a Likert scale to define multiple levels of relevance. In this thesis, no distinction is made between different levels of non-relevance, e.g., there is no differentiation between \glqq not relevant\grqq{} and \glqq strongly not relevant\grqq{}. This standardization was applied because finer distinctions among non-relevant documents were not deemed necessary for the research objectives.
\\\\
At this stage, all documents with at least one relevance assessment for any query of the retrieval task have been identified. Some retrieval tasks, such as \texttt{Robust04} with over 300,000 relevance assessments, contain a large number of assessments per query. As previously mentioned, only a representative subset of judged documents for each query is used for relevance transfer. To limit the number of judged documents in the transfer process, a maximum of 50 relevance assessments per relevance label of a retrieval task for each query, reffered to as query-label combination, is set. This reduces the number of used judged documents from the source corpus and decreases the processing effort required for the transfer pipeline.
\\\\
Additionally, the number of documents for each query-label combination is capped at the minimum number of judged documents across all possible labels for that query. For example, if query $q$ has relevance assessments with label $0$, $1$, and $2$, and there are 30 relevance assessments with label $0$, 40 with label $1$, and 60 with label $2$, then 30 relevance assessments are selected for each of the three labels. This prevents bias toward labels with a higher number of assessments, ensuring a balanced representation of relevance labels within a query.

% Segmentation with spaCy
\subsection{Segmentation with spaCy}\label{segmentation-with-spacy}

Now that the documents used for relevance transfer have been selected, they are segmented into passages. This segmentation is performed to focus only on the most relevant parts of a document rather than the entire text and to ensure proper processing of text snippets through LLMs later. To achieve accurate segmentation with correct sentence separation, the GitHub repository \texttt{grill-lab/trec-cast-tools}\footnote{https://github.com/grill-lab/trec-cast-tools} was utilized. This repository provides a collection of scripts designed to process TREC CAsT Tracks \textcolor{red}{CITEP}. Among its features is the ability to process a document collection and generate passage splits.
\\\\
\texttt{trec-cast-tools} leverages \texttt{spaCy}\footnote{https://spacy.io}, a powerful natural language processing library in Python \textcolor{red}{CITEP BOOK}. \texttt{spaCy} offers various features, including tokenization, part-of-speech (POS) tagging, named entity recognition (NER), and lemmatization. It also provides functionality for sentence segmentation, which is used by \texttt{trec-cast-tools}. First, the documents are processed by \texttt{spaCy}, splitting the texts into sentences. Then, \texttt{trec-cast-tools} concatenates these sentences into passages, with each passage limited to a maximum length of 250 words.
\\\\
As a result, each document is transformed into a series of uniquely identifiable passages which are saved in the \texttt{passage store}, see [2] Figure~\ref{fig:document-segmentation}. These identifiers consist of the original document ID combined with an additional passage ID, ensuring traceability throughout the transfer pipeline.
\\\\
The selected documents from the source corpora are now available as passages. In cases where a document is judged as relevant to a query, the relevant information is often dispersed throughout the text. Therefore, in the next stage of the transfer pipeline, these individual passages are classified to identify those passages with a high density of relevant information. This step optimizes the results of the subsequent candidate selection and enhances the effectiveness of the pairwise preference process at the end of the pipeline.


% ===============
% Passage Scoring
% ===============
\section{Passage Scoring}\label{passage-scoring}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{./graphics/drawio/passage_scoring.png}
    \caption{Generalised visualisation of the passage scoring process by submitting a passage as a query and retrieving a document ranking from the source corpus.}
    \label{fig:passage-scoring}
\end{figure}

To determine which passages of a document are most relevant and best reflect the label of its relevance assessment, a ranking of the individual passages is performed in this step.
\\\\
As detailed in Section \ref{document-selection}, each query-label combination of a retrieval task has an associated set of relevance assessments. To rank all passages of the selected documents, the following procedure is applied to each relevance assessment which contains a selected document. First, each passage of a document is treated as an independent query, as shown in step [1, 2] Figure~\ref{fig:passage-scoring}. This query is then used to retrieve a document ranking from its original source \texttt{document store}, as shown in [3] Figure~\ref{fig:passage-scoring}. Based on the retrieved document ranking for the submitted passage, the relevance of the passage is determined by computing its \texttt{precision@10} and \texttt{nDCG@10} scores, step [4] Figure~\ref{fig:passage-scoring}. These metrics were chosen because they are widely used in information retrieval research. Additionally, \texttt{nDCG} provides a more fine-grained evaluation by considering relevance labels of the retrieved documents, whereas \texttt{precision} only accounts for binary relevance.
\\\\
\textbf{Precision} measures the fraction of retrieved documents that are relevant to a information need. In this context, the requested information is the original query associated with the relevance assessment to which the document of a passage belongs. A retrieved document is considered relevant if it has been judged as such in the relevance assessments of the retrieval task. To simplify passage scoring, the evaluation is restricted to the top 10 retrieved documents. The resulting \texttt{precision@10} score is then assigned as the ranking for the passage. This ranking represents the passage's relevance to the query of its original relevance assessment of the document it belongs to.
\\\\
\textbf{Normalized Discounted Cumulative Gain} (\texttt{nDCG}) is another metric commonly used in information retrieval to evaluate the quality of a ranking. Cumulative Gain (\texttt{CG}) represents the sum of all relevance labels for the retrieved document ranking. Again, the query of the retrieval task to which the document is assigned serves for determining the labels of the retrieved documents. Unlike \texttt{precision}, \texttt{CG} considers the label values instead of simply differentiating between non-relevant and relevant labels. This provides greater granularity in retrieval tasks with more than two labels, enabling more nuanced scoring of the passages of a document. The advanced Discounted Cumulative Gain (\texttt{DCG}) further refines this evaluation. It introduces a positional factor to the ranking. It assigns higher weight to relevant results that appear earlier in the ranking. The final score, \texttt{nDCG}, is calculated by normalizing the \texttt{DCG} score with the \texttt{Ideal DCG} (\texttt{IDCG}), which represents the optimal theoretical ranking for the query. As for \texttt{precision}, the evaluation is limited to the first 10 documents by calculating \texttt{nDCG@10}.
\\\\
The idea behind ranking document passages is that passages containing a high density of relevant information to an information need (i.e., the original query of a relevance assessment) are more likely to retrieve relevant documents and therefore achieve higher \texttt{precision@10} and \texttt{nDCG@10} scores. Conversely, passages with minimal or no relevant information will lead to lower scores due to fewer retrieved relevant documents.
\\\\
At this stage of the pipeline, the individual passages of the selected documents have been scored. These scores will serve as the foundation for the subsequent steps in identifying candidate documents from the target corpora. The selected candidates will receive newly inferred relevance assessments at the end of the pipeline. Additionally, these scores will help determine the most relevant passages from the source corpora, which will be used in pairwise preference comparisons alongside the selected candidates.


% ====================
% Candidate Retreiaval
% ====================
\section{Candidate Selection}\label{candidate-selection}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{./graphics/drawio/candidate_selection.png}
    \caption{Tested approaches for the selection of documents from the target corpus for which relevance labels are to be determined.}
\end{figure}

The next step in the transfer pipeline is selecting candidates for pairwise preference inference at the end of the pipeline. This step consists of two parts. First, for each query in a retrieval task, documents from the target corpus are selected. New relevance assessments will be created for these queries and selected documents. The second part involves choosing passages from the source corpus for each query. These passages will be used in pairwise preference inference to determine whether a document from the target corpus is relevant to a given query.

% Document Retrieval from Target Dataset
\subsection{Document Retrieval from Target Dataset}\label{document-retrieval-from-target-dataset}

For each query in a retrieval task from the source dataset, a set of documents from the target corpus must be selected for which new relevance labels will be inferred. The goal is to identify documents that are most likely relevant to a query. By pre-selecting potentially relevant documents, the number of identified relevant documents at the end of the pipeline can be increased.
\\\\
To achieve this, all three tested approaches for selecting target documents begin with \texttt{BM25} retrieval as a pre-selection method. By submitting texts or passages, that are already identified as relevant for a query, against the target document store, a ranking of target documents is retrieved. The highest-ranked documents are the most likely to be relevant and therefore qualify for relevance label assignment. A fine-grained classification of relevance is performed later through pairwise preference inference. \texttt{BM25} retrieval was chosen for its efficiency \textcolor{red}{REF?} and because only an initial rough assessment is required.
\\\\
\textbf{Naive} The first approach, termed \texttt{naive}, submits each original query text from a retrieval task to the target corpus to retrieve potentially relevant documents. From the resulting document ranking, the top 1000 documents are selected as candidates. Unfortunately, some queries may be ambiguous, leading to multiple possible interpretations. For example, the query \glqq Apple\grqq{} could refer to either the technology company or the fruit. To address such ambiguities, the query description is also submitted as an independent query. A query description is a brief text that provides additional context, clarifying the search intent and specifying a query's focus. This retrieves another 1000 documents via \texttt{BM25} ranking. After filtering out duplicates between the two sets, up to 2000 unique documents per query are selected.
\\\\
\textbf{Nearest Neighbor} The second approach, \texttt{nearest neighbor}, is based on the relevant passages identified in Section~\nameref{passage-scoring}. In thies strategy, the top-scored passages for each query are submitted as queries to the target corpus. From the retrieved document ranking for each passage, the top 20 documents are selected as candidates for the corresponding query. As a result, each passage can contribute up to 20 unique documents to the candidate set.
\\\\
Different variations of the \texttt{nearest neighbor} approach were tested. First, the number of selected top passages per query was limited. One variant used the top 10 passages, another used the top 50, and a third used the top 100 passages. This limitation significantly reduces the number of possible candidates, as the number of relevant passages can be exceedingly large for a single query. The second variation imposed a restriction allowing only one passage per original document to be used for retrieval, ensuring greater diversity. Conversely, an alternative approach permitted multiple passages from the same document to be used. This rule was tested to assess the impact of passage variation in retrieval.
\\\\
\textbf{Union Approach} The third approach, called \texttt{union}, combines the \texttt{naive} and \texttt{nearest neighbor} approaches into a single candidate set for each query. This combination is intended to enhance the \texttt{Recall} of the number of retrieved relevant docuements by including a broader set of potentially relevant documents. However, the increased number of documents may reduce \texttt{Precision} by including more irrelevant entries, thereby increasing the workload for pairwise preference evaluations in later stages. The \texttt{Recall} and \texttt{Precision} of the approaches and variations are evaluated in Chapter~\ref{evaluation}~[\nameref{candidate-selection}].

% Postprocessing of Selected Target Documents
\subsection{Postprocessing of Selected Target Documents}\label{postprocessing-of-selected-target-documents}

As described in Section~\nameref{segmentation-with-spacy}, selected documents from the source dataset are segmented into passages for later processing. Since pairwise preferences will be applied at passage level, documents from both the source and target datasets have to be divided into passages. This is done using the same process as outlined before. First, \texttt{spaCy} is employed for segmenting the chosen target documents into sentences, and then passages are formed by concatenating sentences.

% Composing Final Candidates
\subsection{Composing Final Candidates}\label{composing-final-candidates}

For pairwise preference inference, a query, a passage from a document in the target corpus, and a known passage from the source corpus are needed (see \ref{fig:pairwise-preferences}). The queries are provided directly by the retrieval tasks. Now that the target documents have been identified, the selection of passages from the source dataset for pairwise preference inference can proceed. The goal is to compare each selected passage from the target corpus for a given query against 15 relevant and 5 non-relevant passages from the source corpus. To determine relevant and non-relevant passages, the computed passage scores from Section~\nameref{passage-scoring} are used.
\\\\
\textbf{Simple Selection} During the \nameref{passage-scoring} stage, each selected passage from the source corpus is evaluated and assigned scores based on the \texttt{precision@10} and \texttt{nDCG@10} metrics. These scores are now used to generate a ranking for each query. Following this strategy, the 15 highest-scoring passages and the 5 lowest-scoring passages for each query are selected.
\\\\
\textbf{Diversified Selection} It is possible that the top- and lowest-rated passages are predominantly drawn from a small subset of documents. While such passages may satisfy the selection criteria of the \texttt{simple selection}, this concentration could lead to unintended side effects by limiting variation in the pairwise preferences. To mitigate potential bias because of reduced document diversity, this approach restricts the selection to a maximum of one passage per document. Specifically, it selects the top 15 and bottom 5 passages for each query, ensuring that each passage is sourced from a distinct document. This strategy enhances diversity in pairwise preference comparisons by incorporating a wider range of source documents.
\\\\
For each query in a retrieval task, 20 selected passages, either from the simple or diversified selection approach, are chosen. These passages serve as a comparison standard in pairwise preference inference to determine the relevance of passages from the target corpus.


% ====================
% Pairwise Preferences
% ====================
\section{Pairwise Preferences}\label{pairwise-transfering-relevance-labels-across-datasets}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{./graphics/drawio/pairwise_preferences.png}
    \caption{image}
    \label{fig:pairwise-preferences}
\end{figure}

At this stage, all necessary processing steps have been completed to perform the pairwise preferences. The transfer pipeline began with selecting a subset of documents from the source corpus for each query. These documents were segmented into passages, and each passage was scored by treating it as an independent query against the source corpus. The retrieved document rankings were used to compute \texttt{precision} and \texttt{nDCG} metrics, which were then assigned as passage scores. The highest-scoring passages guided the selection of candidates for the pairwise preferences, which is now described in detail.
\\\\
A candidate is defined as a combination of three elements: a query from a retrieval task of the source dataset, a known passage from a source dataset document, and a passage to judge from a target dataset document. The query is predefined by the retrieval task, while the known passage was identified during the initial document selection step. The passage to judge was determined in the candidate retrieval step. The known passage serves as a reference for comparison and can either be relevant or non-relevant. The objective of the pairwise preference evaluation is to assess the relevance of the passage to judge with respect to the query by comparing it to the known passage.
\\\\
The \texttt{autoqrels}\footnote{https://github.com/seanmacavaney/autoqrels} GitHub repository was used to perform pairwise preference. \texttt{autoqrels} is a tool designed to automatically infer query relevance assessments (qrels), supporting zero-shot and one-shot labeling. The original \texttt{autoqrels} paper~\citep{macavaney:2023} evaluated the capability of one-shot labelers for automatic relevance estimation. Among the evaluated approaches, the \texttt{DuoPrompt} method which uses the \texttt{FLAN-T5} model demonstrated superior performance compared to the other tested systems \texttt{DuoT5}, {MaxRep-TCT} and \texttt{MaxRep-TCT}. Therefore, for this thesis, the one-shot labeling with \texttt{DuoPrompt} was utilized to perform the pairwise preference inference.
\\\\
\texttt{FLAN-T5} is a Text-to-Text Transfer Transformer model that is an advanced version~\citep{chung:2022} of Google's \texttt{T5} model. While the backbone architecture of \texttt{FLAN-T5} remains \texttt{T5}, it has been further fine-tuned on an expansive and diverse set of training tasks, enhancing its performance across a wide range of natural language processing tasks. \texttt{FLAN-T5} is available in various sizes, ranging from smaller, resource-efficient versions like \texttt{flan-t5-small}, with 80~million parameters, to much larger models such as \texttt{flan-t5-xxl} with 11~billion parameters, accommodating different computational and application needs. For this thesis, given the constraints in computational resources, the \texttt{flan-t5-base}\footnote{https://huggingface.co/google/flan-t5-base} model with 250 million parameters is used as the DuoPrompt model.
\\\\
\begin{figure}[ht]
    \centering
    \begin{tcolorbox}[title=One-Shot Prompt, width=0.95\textwidth]
        \footnotesize
        \begin{verbatim}
PROMPT = (
    "Determine if passage B is as relevant as passage A "
    "for the given query. "
    'Passage A: "...{{ rel_doc_text | replace("\\"", "\'") }}..." '
    'Passage B: "...{{ unk_doc_text | replace("\\"", "\'") }}..." '
    'Query: "{{ query_text }}" '
    "Is passage B as relevant as passage A? </s>"
)
        \end{verbatim}
    \end{tcolorbox}
    \caption{One-shot prompt used for the \texttt{FLAN-T5} model.}
    \label{fig:oneshot-prompt}
\end{figure}

In Figure~\ref{fig:oneshot-prompt} is the prompt structure which is given to \texttt{FLAN-T5}. The prompt takes the query, the known passage (referred to as rel\_doc\_text), and the passage to judge (referred to as unk\_doc\_text) as inputs for pairwise inference. The model then predicts the likelihood that the passage to judge is as relevant to the query as the known passage. If the passage to judge is highly relevant to the query, the model outputs a score close to 1.0. Conversely, if the passage to judge is not relevant, the model produces a score close to 0.0. This process enables a detailed comparison of passage relevance, forming the foundation for inferring new relevance assessments for documents in the target dataset.
\\\\
All candidates are processed through pairwise preference inference, where each passage to judge is compared against 20 known passages as outlined in Section~\ref{composing-final-candidates}. The pairwise preference inferences generate 20 relevance scores for each passage to judge, with higher scores indicating greater relevance to the query associated with the candidate.
\\\\
This chapter provided a detailed description of all steps in the transfer pipeline, resulting in a set of relevance scores for each passage to judge. These scores represent the transferred information from the source dataset, specifically the qrels of a retrieval task, to the target dataset. The next chapter will evaluate the effectiveness of the transfer pipeline and its individual steps.
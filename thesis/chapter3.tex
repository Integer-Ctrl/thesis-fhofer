\chapter{Methodology}\label{methodology}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./graphics/drawio/transfer_pipeline.pdf}
    \caption{Overview of the transfer pipeline and its individual steps to transfer exisiting relevance judgments into another document corpus.}
    \label{fig:transfer-pipeline}
\end{figure}

In this Chapter, I present the individual steps of the transfer pipeline used for relevance transfer. The process begins with an information retrieval task consisting of a document corpus, a set of queries, and corresponding relevance judgments (Figure~\ref{fig:datasets}). The goal of the pipeline is to transfer the existing relevance judgments from a retrieval task to a target document corpus, thereby generating new relevance judgments. A simplified overview of the process is shown in Figure \ref{fig:transfer-pipeline}, which will now be explained in detail.
\\\\
The first step in the transfer pipeline involves selecting and segmenting documents from the retrieval task's document corpus. Only documents that contain at least one relevance judgment are selected, as those without any judgment do not provide useful information for the transfer process. Once selected, these documents are segmented into passages. This segmentation is done because relevant information related to an information need is typically concentrated in specific sections rather than spread across the entire document. By breaking documents into passages, the focus remains on the most relevant content. Additionally, since later stages of the pipeline rely on large language models, limiting the size of processed text through segmentation helps prevent potential issues when handling long documents with transformer models.
\\\\
The second step focuses on identifying the most relevant passages within each selected document. Therefore, the relevance of each passage is determined in respect to the query of its documents relevance judgment. To achieve this, each passage is treated as an independent query and submitted to the source document corpus. The resulting document ranking is then used to compute various evaluation metrics, which are assigned as passage scores. These scores are later used for selecting candidate documents from the target document corpus and identifying source passages for pairwise preference comparisons.
\\\\
The third step is the selection of candidate documents from the target document corpus. For each query in a retrieval task, a set of documents is chosen to receive new relevance judgments. This selection is performed using different strategies with the aim to identify documents most likely to be relevant. Each selected target document is then paired with relevant passages from the source dataset, identified in the previous step. These candidates serve as input for the pairwise inference process in the next step.
\\\\
At this stage, the transfer pipeline has identified, for each query, a set of target documents along with a corresponding set of relevant passages from the source dataset. The final step is inferring new relevance judgments for the target documents. To achieve this, target documents are also segmented into passages to be processed with source passages. Each query is then processed alongside a target passage and a source passage using a pairwise ranking model to determine whether the target passage is as relevant as the source passage. Finally, a label for an entire document from the target document corpus is then created by aggregating the relevance scores of all its passages.

% ========
% Datasets
% ========
\section{Datasets}\label{datasets}

\begin{table}[t]
    \centering
    \setcitestyle{numbers}
    \footnotesize
    % \caption{List of source datasets and their associated retrieval tasks from which existing \texttt{qrels} where transferd into the target document corpus \texttt{ClueWeb22/b}.}
    \caption{Source datasets and their retrieval tasks from which query relevance judgments (\texttt{qrels}) were transferred to the target document corpus \texttt{ClueWeb22/b}.}
    % \caption{List of source datasets with their associated retrieval tasks, from which existing query relevance judgments (\texttt{qrels}) were transferred to the target document corpus \texttt{ClueWeb22/b}.}
    \label{tab:datasets}
    \begin{tabular}{cccrrc}
        \toprule
        \multicolumn{2}{c}{\textbf{Corpus}} & \multicolumn{4}{c}{\textbf{ Associated Retrieval Tasks}} \\
        \cmidrule(lr){1-2} \cmidrule(lr){3-6}
        Name & documents  & Name & Queries &\texttt{qrels} & Labels \\
        \toprule
        
        Args.me~\cite{ajjour:2019} & 0.4~m & Touch√© 2020 Task 1~\cite{bondarenko:2020} & 49 & $2\,298$ & 3 \\
        \midrule

        \multirow{3}{*}{Disks4+5~\cite{voorhees:1996}} & \multirow{3}{*}{0.5~m} & Robust04~\cite{voorhees:2004} & 250 & $311\,410$ & 3 \\
        & & TREC-7~\cite{voorhees:1998} & 50 & $80\,345$ & 2 \\
        & & TREC-8~\cite{voorhees:1999} & 50 & $86\,830$ & 2 \\
        \midrule

        \multirow{2}{*}{MS MARCO Passage~\cite{bajaj:2016}} & \multirow{2}{*}{8.8~m} & TREC 2019 DL Track~\cite{craswell:2019} & 43 & $9\,260$ & 4 \\
        & & TREC 2020 DL Track~\cite{craswell:2020} & 54 & $11\,386$ & 4 \\
        
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./graphics/drawio/datasets.pdf}
    \caption{Simplified structure of a dataset from \texttt{ir\_datasets}, comprising a set of documents, queries, and relevance judgments, along with their attributes.}
    \label{fig:datasets}
\end{figure}

Before the actual processing can begin, it is necessary to determine the source and target of the relevance transfer. This involves selecting datasets, along with their associated information retrieval tasks, as the foundation for transferring relevance judgments to a target document corpus.
\\\\
To simplify data handling, I used \texttt{ir\_datasets}~\citep{macavaney:2021}, a Python package that provides access to numerous information retrieval datasets. As illustrated in Figure~\ref{fig:datasets}, each dataset consists of a document corpus, a query store, and a set of relevance judgments (\texttt{qrels}) for the queries and documents.
\\\\\\
A key advantage of \texttt{ir\_datasets} is its standardized interface\footnote{https://ir-datasets.com/python.html}, which enables uniform access to different datasets. Through built-in iterators, the package facilitates structured access to corpora, queries, and relevance judgments, allowing the transfer pipeline to handle diverse datasets efficiently.
\\\\
The datasets and associated information retrieval tasks used in this thesis are listed in Table~\ref{tab:datasets}. The source datasets were selected based on their widespread use in information retrieval research and their varying sizes. \texttt{Args.me} and \texttt{Disks4+5} are relatively small corpora, whereas \texttt{MS MARCO} is significantly larger. Additionally, the number of existing relevance judgments in these datasets varies considerably. \texttt{Args.me} contains approximately 2,300 judgments, while the retrieval tasks associated with \texttt{MS MARCO} have several thousand. In contrast, the tasks associated with \texttt{Disks4+5} extend far beyond this, with one exceeding 300,000 judgments. Notably, the \texttt{MS MARCO} dataset is already preprocessed and provided as passages, unlike the other datasets. This is a significant difference, as the relevance judgments are already at the passage level, eliminating the need for further segmentation of the documents. This diversity in dataset size and relevance judgment density provides a robust test for the transfer process under different conditions.
% The datasets and associated information retrieval tasks used in this thesis are listed in Table~\ref{tab:datasets}. The source datasets were selected based on their wide use in information retrieval research and their varying sizes. \texttt{Args.me} and \texttt{Disks4+5} are relatively small corpora, whereas \texttt{MS MARCO} is significantly larger. Additionally, the number of existing relevance judgments in these datasets varies considerably. \texttt{Args.me} contains approximately 2,300 judgments, while the retrieval tasks associated with \texttt{MS MARCO} have several thousand. In contrast, the tasks of \texttt{Disks4+5} extend far beyond this, with one exceeding 300,000 judgments. This diversity in dataset size and relevance judgment density provides a robust test for the transfer process under different conditions.
\\\\
The selected datasets serve as the starting point of the pipeline, while the final target corpus for relevance transfer is \texttt{ClueWeb22/b}. This dataset was chosen because it is the newest ClueWeb corpus in the \texttt{Lemur Project}\footnote{https://lemurproject.org}. With over 1.0~billion documents, \texttt{ClueWeb22/b} is significantly large, but due to its recent release, it currently has a low number of relevance judgments. The objective of the transfer process is to enrich \texttt{ClueWeb22/b} by leveraging the existing relevance judgments from the source datasets, thereby enhancing its use cases for information retrieval research.
\pagebreak


% =====================
% Document Segmentation
% =====================
\section{Document Segmentation}\label{document-segmentation}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./graphics/drawio/document_segmentation.pdf}
    \caption{Visualization of the document segmentation step in the process pipeline. First, a subset of documents is selected from the document store. Then, the selected documents are segmented using \texttt{spaCy}.}
    \label{fig:document-segmentation}
\end{figure}

With the selection of the source datasets, their retrieval tasks, and the target dataset completed, the actual processing of the transfer pipeline can begin. To provide a clearer understanding of the subsequent steps in the pipeline, the following explanations will use one of the datasets listed in Table~\ref{tab:datasets} along with its associated retrieval task as an illustrative example. The described steps apply uniformly across all datasets and retrieval tasks.
\\\\
The first step involves selecting and segmenting documents from the source dataset into passages, as illustrated in Figure~\ref{fig:document-segmentation}. The primary objective of the pipeline is to transfer the relevance judgments from the source retrieval tasks to the target corpus. This transfer starts by identifying the relevant documents in the source corpus based on the existing relevance judgments for each query in the retrieval task. These documents serve as the foundation for the later pairwise preference step, where they are compared with documents from the target corpus. To reduce computational overhead, only a subset of documents from the source dataset is selected. As demonstrated by \mbox{\citet{gienapp:2022}}, the number of pairwise preferences can be reduced through sampling without significantly impacting effectiveness. Consequently, the pipeline will utilize a limited set of comparison documents rather than all possible documents, minimizing the computational cost. Once the relevant documents have been selected, they are segmented into passages. This segmentation is essential because relevant information related to a query is often confined to specific sections rather than the entire document. By processing documents at the passage level, the pipeline can focus on the most relevant parts while filtering out less relevant parts of a document. Additionally, the subsequent steps of the pipeline involve processing text with transformer models, which have a maximum input length they can handle. Segmenting documents into smaller passages ensures compliance with these constraint, thereby enabling the models to operate effectively \citep{levy:2024}.

% Now that the source datasets, along with their retrieval tasks and the target dataset, have been selected, the actual processing can begin. The first step in the transfer pipeline involves selecting and segmenting documents from a source dataset into passages as shown in Figure~\ref{fig:document-segmentation}. The primary objective is to transfer existing relevance judgments from the source retrieval task to the target corpus. This is done by using relevant documents, given through the existing relevance judgments, for each query in the retrieval task. These documents are the basis for comparison in the later pairwise preference step, alongside with documents from the target dataset. To reduce the computational effort required for the transfer pipeline, only a subset of documents from the source dataset is selected. Based on the results of \citep{gienapp:2022}, the number of pairwise preferences can be reduced while maintaining efficiency. Therefore the later pairwise preferences will be done using some comparison documents, and not all possible documents from the source document corpus. This decreases the processing effort required for the transfer pipeline. Once the documents are chosen, they are segmented into passages, as relevance is often confined to specific sections rather than the entire document text. By breaking documents into passages, the pipeline can focus on the most relevant content while disregarding non-relevant parts of a document in the subsequent processing steps. Furthermore, later stages of the pipeline employ transformer models, which have a maximum input length they can process. Segmenting documents before passing them through LLMs ensures compliance with this constraint, allowing the models to work effectively \citep{levy:2024}.

% Document Selection
\subsection{Document Selection}\label{document-selection}

Instead of using the entire document corpus for relevance transfer, only a representative selection of documents is used, step [1] in Figure~\ref{fig:document-segmentation}. First, all documents without relevance judgments in the retrieval task can be ignored since they contain no useful information for the relevance transfer. Therefore, only judged documents are considered for selection. A document is considered judged if it has at least one relevance judgment in the \texttt{qrel store}. These judged documents, along with their relevance judgments for one or multiple queries of the retrieval task, can be used to transfer information, as their relevance has already been determined.
\\\\
As shown in Table~\ref{tab:datasets}, each retrieval task uses different relevance labels to indicate the relevance of a document to a given query. Some datasets apply binary labels, distinguishing simply between relevant and non-relevant documents, while others utilize a Likert scale with multiple relevance levels to represent varying degrees of relevance. In this thesis, no distinction is made between different levels of non-relevance. For instance, \glqq not relevant\grqq{} (\texttt{label = 0}) and \glqq strongly not relevant\grqq{} (\texttt{label < 0}) are treated equally. This standardization was applied because finer distinctions between non-relevant documents are not considered necessary, as many TREC evaluation implementations and metrics such as \texttt{NDCG} map negative labels to zero \citep{gienapp:2020}.
% As shown in Table~\ref{tab:datasets}, each retrieval task uses different relevance labels to indicate the relevance of a document to a given query. Some datasets apply binary labels, distinguishing simply between relevant and non-relevant documents. Others utilize a Likert scale, offering multiple relevance levels to represent varying degrees of relevance. In this thesis, no distinction is made between different levels of non-relevance. For instance, \glqq not relevant\grqq{} (\texttt{label = 0}) and \glqq strongly not relevant\grqq{} (\texttt{label < 0}) are treated equally. This standardization was done because finer distinctions between non-relevant documents were not considered necessary as the transfer pipeline focuses on relevant documents.
% The goal of this work is to transfer relevance judgments, which are assessments made by human annotators who assign relevance labels to documents based on a given query. As shown in Table~\ref{tab:datasets}, each retrieval task categorizes relevance judgments using different relevance labels. A relevance label specifies how relevant a document is to an information need, i.e., a query. This categorization varies in granularity. Some tasks use binary labels to distinguish only between relevant and non-relevant documents, while others employ a \mbox{Likert} scale to define multiple levels of relevance. In this thesis, no distinction is made between different levels of non-relevance, e.g., there is no differentiation between \glqq not relevant\grqq{} and \glqq strongly not relevant\grqq{}. This standardization was applied because finer distinctions among non-relevant documents were not deemed necessary for the research objectives.
\\\\\\\\\\
At this stage, all documents with at least one relevance judgment for any query of the retrieval task have been identified. Some retrieval tasks, such as \texttt{Robust04} with over 300,000 relevance judgments, contain a large number of judgments per query. As mentioned above, only a representative subset of judged documents can be used for relevance transfer for each query. To limit the number of judged documents in the transfer process, a maximum of 50 relevance judgments per relevance label of a retrieval task for each query, referred to as a query-label combination, is selected.
%\\\\
%Additionally, the number of documents for each query-label combination is capped at the minimum number of judged documents across all possible labels for that query. For example, if query $q$ has relevance judgments with label $0$, $1$, and $2$, and there are 30 relevance judgments with label $0$, 40 with label $1$, and 60 with label $2$, then 30 relevance judgments are selected for each of the three labels. The idea is to prevent bias toward labels with a higher number of judgments, ensuring a balanced representation of relevance labels for a query.

% Segmentation with spaCy
\subsection{Segmentation with spaCy}\label{segmentation-with-spacy}

Now that the documents used for relevance transfer have been selected, they are segmented into passages. This segmentation is performed to focus only on the most relevant parts of a document rather than the entire text and to ensure proper processing of text snippets through large language models later. To achieve accurate segmentation with correct sentence separation, the GitHub repository \texttt{grill-lab/trec-cast-tools}\footnote{https://github.com/grill-lab/trec-cast-tools} was utilized. The repository provides a collection of scripts designed to process TREC CAsT Track 2022~\citep{owoicho:2022}. Among its features is the ability to process a document collection and generate passage splits.
\\\\
\texttt{trec-cast-tools} leverages \texttt{spaCy}\footnote{https://spacy.io}, a powerful natural language processing library in Python. \texttt{spaCy} offers various features, including tokenization, part-of-speech (POS) tagging, named entity recognition (NER), and lemmatization. It also provides functionality for sentence segmentation, which is used by \texttt{trec-cast-tools}. First, the documents are processed by \texttt{spaCy}, splitting the texts into sentences. Then, \texttt{trec-cast-tools} concatenates these sentences into passages, with each passage limited to a maximum length of 250 words.
\\\\    
The selected documents from the source corpus are now available as uniquely identifiable passages, ensuring traceability throughout the transfer pipeline, which are saved in the \texttt{passage store}, see [2] Figure~\ref{fig:document-segmentation}. In cases where a document is judged as relevant to a query, the relevant information is often dispersed throughout the text. Therefore, the documents were segmented into passages. In the next stage of the transfer pipeline, these individual passages are classified to identify those passages with a high density of relevant information. This step optimizes the results of the subsequent candidate selection and enhances the effectiveness of the pairwise preference process at the end of the pipeline by using highly relevant passages for comparison.
\pagebreak


% ===============
% Passage Scoring
% ===============
\section{Passage Scoring}\label{passage-scoring}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./graphics/drawio/passage_scoring.pdf}
    \caption{Overview of the passage scoring process, where each passage is submitted as an independent query to its source document corpus, and scores are assigned based on the retrieved document ranking.}
    \label{fig:passage-scoring}
\end{figure}

As selected in Section \ref{document-selection}, each query-label combination of a retrieval task has an associated set of relevance judgments along with their corresponding documents. To determine which passages of a document are most relevant and best reflect the label of its relevance judgment, a ranking of the individual passages is performed in this step. To rank all passages of the selected documents, the following procedure is applied to all selected documents. First, each passage of a document is treated as an independent query, as shown in step \mbox{[1, 2] Figure~\ref{fig:passage-scoring}}. This query is then used to retrieve a document ranking from its original source \texttt{document store}, as shown in step [3] Figure~\ref{fig:passage-scoring}. Based on the retrieved document ranking for the submitted passage, the relevance of the passage is determined by computing its \texttt{precision@10} and \texttt{nDCG@10} scores, step [4] Figure~\ref{fig:passage-scoring}. These metrics were chosen because they are widely used in information retrieval research. In contrast to \texttt{precision}, \texttt{nDCG} provides a more fine-grained evaluation by considering relevance labels of the retrieved documents, whereas \texttt{precision} only accounts for binary relevance.
\\\\
\textbf{Precision} is the fraction of retrieved documents that are relevant to a information need. In this context, the information need corresponds to the original query associated with the relevance judgment of the document to which a passage belongs. A retrieved document is considered relevant if it has been judged as such in the relevance judgments for that query. To simplify the scoring process, the evaluation is restricted to the top 10 retrieved documents. The resulting \texttt{precision@10} score is then assigned as the passage's score, representing its relevance to the query of the document's original relevance judgment.
\\\\
\textbf{Normalized Discounted Cumulative Gain} (\texttt{nDCG}) is another metric commonly used in information retrieval to evaluate the quality of a retrieved document ranking. Similar to \texttt{precision}, the query associated with the relevance judgment of the document to which a passage belongs is used to determine the relevance labels of the retrieved documents. Cumulative Gain (\texttt{CG}) represents the sum of all relevance labels for the retrieved document ranking. Unlike \texttt{precision}, \texttt{CG} takes into account the label values rather than simply differentiating between relevant and non-relevant labels. This provides greater granularity in retrieval tasks with more than two relevance levels, allowing for more nuanced scoring of document passages. The advanced Discounted Cumulative Gain (\texttt{DCG}) further refines this evaluation by introducing a positional factor to the ranking, assigning higher weight to relevant results that appear earlier in the ranking. The final \texttt{nDCG} score is calculated by normalizing the \texttt{DCG} score with the Ideal DCG (\texttt{IDCG}), which represents the optimal theoretical document ranking for the query. As with \texttt{precision}, the evaluation is limited to the first 10 documents by only computing \texttt{nDCG@10}.
\\\\
The idea behind ranking a document's passages is that passages containing a high density of relevant information to an information need (i.e., the original query of a relevance judgment) are more likely to retrieve relevant documents and therefore achieve higher \texttt{precision@10} and \texttt{nDCG@10} scores. Conversely, passages with minimal or no relevant information will lead to lower scores due to fewer retrieved relevant documents.
\\\\
At this stage of the pipeline, the individual passages of the selected documents have been scored. These scores will serve as the foundation for the subsequent steps in identifying candidate documents from the target corpus. The selected candidates will receive newly inferred relevance judgments at the end of the pipeline. Additionally, these scores will help determine the most relevant passages from the source document corpus, which will be used in pairwise preference comparisons alongside the selected candidates.


% ====================
% Candidate Retreiaval
% ====================
\section{Candidate Selection}\label{candidate-selection}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./graphics/drawio/candidate_selection.pdf}
    % \caption{Tested approaches for the selection of documents from the target corpus for which relevance labels are to be determined.}
    \caption{Overview of the three candidate retrieval approaches for selecting documents from the target corpus. The selected candidates will receive new relevance labels inferred at the end of the pipeline.}
\end{figure}

The next step in the transfer pipeline is selecting candidates for pairwise preference inference. This step consists of two parts. First, for each query in a retrieval task, documents from the target corpus are selected. New relevance judgments will be created for these queries and their selected documents. The second part involves choosing passages from the source corpus for each query. These passages will be used in pairwise preference comparison to determine whether a document from the target corpus is relevant to a given query.

% Document Retrieval from Target Dataset
\subsection{Document Retrieval from Target Dataset}\label{document-retrieval-from-target-dataset}

For each query in a retrieval task from the source dataset, a set of documents from the target corpus must be selected for which new relevance labels will be inferred. The goal is to identify documents that are most likely relevant to a query. By pre-selecting potentially relevant documents, the number of identified relevant documents at the end of the pipeline should be increased.
\\\\
To achieve this, all three tested approaches for selecting target documents begin with \texttt{BM25} retrieval as a pre-selection method. By submitting texts or passages, that are already identified as relevant for a query, against the target document store, a ranking of target documents is retrieved. The highest-ranked documents are most likely to be relevant and therefore qualify for relevance label assignment. A fine-grained classification of relevance is performed later through the actual pairwise preference inference. \texttt{BM25} retrieval was chosen for its fast retrieval and because only an initial rough assessment is required.
\\\\
\textbf{Naive} The first approach, termed \texttt{naive}, submits each original query text from a retrieval task to the target corpus to retrieve potentially relevant documents. From the resulting document ranking, the top 1000 documents are selected as candidates. Unfortunately, some queries may be ambiguous, leading to multiple possible interpretations. For example, the query \glqq Apple\grqq{} could refer to either the technology company or the fruit. To address such ambiguities, the query description is also submitted as an independent query. A query description is a brief text that provides additional context, clarifying the search intent and specifying a query's focus. This retrieves another 1000 documents via \texttt{BM25} ranking. After filtering out duplicates between the two sets, up to 2000 unique documents per query are selected.
\\\\
\textbf{Nearest Neighbor} The second approach, \texttt{nearest neighbor}, is based on the relevant passages identified in [Section~\ref{passage-scoring}]. In this strategy, the top-scoring passages for each query are submitted as queries to the target corpus. For each passage, the top 20 documents in the retrieved document ranking are selected as candidates for the corresponding query. As a result, each passage can contribute up to 20 unique documents to the candidate set. Additionally, several variations of the \texttt{nearest neighbor} approach were tested to determine the most effective candidate selection strategy. First, the number of top passages per query from the source dataset was limited. Three variations were evaluated: one using the top 10 passages, another using the top 50, and a third using the top 100 passages. This limitation significantly reduces the number of potential candidates, as a single query can have an exceedingly large number of relevant passages. The second variation is restricting the selection to one passage per source document. For example, if the top 10 passages for a query contained multiple passages from the same document, only the best-scoring passage is used for retrieving. The intention behind this restriction is to ensure greater diversity in the retrieved candidates. Conversely, an alternative approach permitted multiple passages from the same document. This variation was tested to assess the impact of passage diversity on retrieval effectiveness.
% \textbf{Nearest Neighbor} The second approach, \texttt{nearest neighbor}, is based on the relevant passages identified in [Section~\ref{passage-scoring}]. In thies strategy, the top-scored passages for each query are submitted as queries to the target corpus. From the retrieved document ranking for each passage, the top 20 documents are selected as candidates for the corresponding query. As a result, each passage can contribute up to 20 unique documents to the candidate set. Several variations of the \texttt{nearest neighbor} approach were tested to determine the most effective candidate selection strategy. First, the number of used top passages per query from the source dataset was limited. Three variations were evaluated: one using the top 10 passages, another using the top 50, and a third using the top 100 passages. This limitation significantly reduces the number of potential candidates, as a single query can have an exceedingly large number of relevant passages. The second variation is a restriction that allowed only one passage per source document to be used for retrieval. That means for example, if the top 10 passages for a query contain multiple passages from the same document, only the best one can contribute to the top 10. The intension of this is ensuring greater diversity in the selected candidates. In contrast, an alternative approach permitted multiple passages from the same document to be used. This variation was tested to assess the impact of passage diversity on retrieval effectiveness.
\\\\
\textbf{Union Approach} The third approach, called \texttt{union}, combines the \texttt{naive} and \texttt{nearest neighbor} approaches into a single candidate set for each query. This combination is intended to enhance the \texttt{Recall} of the number of retrieved relevant docuements by including a broader set of potentially relevant documents. However, the increased number of documents may reduce \texttt{Precision} by including more irrelevant entries, thereby increasing the workload for pairwise preference evaluations in later stages. A detailed evaluation of the various candidate retrieval approaches was done in Chapter~\ref{evaluation} [Section~\ref{eval-candidate-selection}].
% The \texttt{Recall} and \texttt{Precision} of the approaches and variations are evaluated in Chapter~\ref{evaluation}~[\nameref{candidate-selection}].

% Postprocessing of Selected Target Documents
\subsection{Postprocessing of Selected Target Documents}\label{postprocessing-of-selected-target-documents}

As described in [Section~\ref{segmentation-with-spacy}], selected documents from the source dataset are segmented into passages for later processing. Since pairwise preferences will be applied at passage level, documents from both the source and target datasets have to be divided into passages. Therefore, the selected candidate documents are processed with \texttt{trec-cast-tools} as outlined before. First, \texttt{spaCy} is employed for segmenting the chosen target documents into sentences, and then passages are formed by concatenating sentences.

% Composing Final Candidates
\subsection{Composing Final Candidates}\label{composing-final-candidates}

For pairwise preference inference, a query, a passage from a selected document in the target corpus, and a known passage from the source corpus are required, as illustrated in Figure~\ref{fig:pairwise-preferences}. The queries are directly provided by the retrieval tasks. As above described, the target documents have been identified and segmented into passages. What remains is the selection of passages from the source dataset for pairwise preference comparison. The goal is to compare each selected passage from the target corpus, for a given query, against 15 relevant and 5 non-relevant passages from the source corpus.
% For pairwise preference inference, a query, a selected passage from a document in the target corpus, and a known passage from the source corpus are needed, as illustrated in Figure~\ref{fig:pairwise-preferences}. The queries are provided directly by the retrieval tasks. As just described, the target documents have been identified and segmented into passages. Remaining is the selection of passages from the source dataset for pairwise preference comparison. The goal is to compare each selected passage from the target corpus for a given query against 15 relevant and 5 non-relevant passages from the source corpus. To determine relevant and non-relevant passages, the computed passage scores from Section~\nameref{passage-scoring} are used.
\\\\
\textbf{Simple Selection} During the passage scoring stage [Section~\ref{passage-scoring}], all passages of each selected document from the source corpus is evaluated and assigned scores based on \texttt{precision@10} and \texttt{nDCG@10} metrics. These scores are now used to generate a ranking of all passages for each query. Following this strategy, the 15 highest-scoring passages and the 5 lowest-scoring passages for each query are selected for pairwise preference comparison.
\\\\\\\\\\\\\\\\
\textbf{Diversified Selection} It is possible that the top- and lowest-rated passages are predominantly drawn from a small subset of documents. This can occur when multiple passages from the same document receive extremely high or low passage scores. While such passages may meet the selection criteria of the \texttt{simple selection} approach, this concentration could unintentionally reduce the diversity of pairwise preferences. To mitigate potential bias resulting from limited document diversity, this approach restricts the selection to a maximum of one passage per document. Specifically, the approach selects the top 15 and bottom 5 passages for each query, ensuring that each passage originates from a distinct document. This strategy enhances the diversity of pairwise preference comparisons by forcing a broader range of source documents.
% It is possible that the top- and lowest-rated passages are predominantly drawn from a small subset of documents. This is the case if multiple passages of the same document receive a very high or extreme low passage score. While such passages may satisfy the selection criteria of the \texttt{simple selection}, this concentration could lead to unintended side effects by limiting variation in the pairwise preferences. To mitigate potential bias because of reduced document diversity, this approach restricts the selection to a maximum of one passage per document. Specifically, it selects the top 15 and bottom 5 passages for each query, ensuring that each passage is sourced from a distinct document. This strategy enhances diversity in pairwise preference comparisons by incorporating a wider range of source documents.
\\\\
For each query in a retrieval task, 20 selected passages, either from the \texttt{simple selection} or \texttt{diversified selection} approach, are chosen. These passages serve as a comparison standard in pairwise preference inference to determine the relevance of passages from the target corpus.
\pagebreak

% ====================
% Pairwise Preferences
% ====================
\section{Pairwise Preferences}\label{pairwise-preferences}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{./graphics/drawio/pairwise_preferences.pdf}
    \caption{Visualization of the pairwise preference inference process with \texttt{DuoPrompt}. Each candidate consists of a query, a known passage from the source corpus, and a passage to judge from the target corpus.}
    \label{fig:pairwise-preferences}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{tcolorbox}[title=One-Shot Prompt, width=\textwidth]
        \footnotesize
        \begin{verbatim}
PROMPT = (
    "Determine if passage B is as relevant as passage A "
    "for the given query. "
    'Passage A: "...{{ rel_doc_text | replace("\\"", "\'") }}..." '
    'Passage B: "...{{ unk_doc_text | replace("\\"", "\'") }}..." '
    'Query: "{{ query_text }}" '
    "Is passage B as relevant as passage A? </s>"
)
        \end{verbatim}
    \end{tcolorbox}
    \caption{One-shot prompt used for the \texttt{DuoPrompt} models in Figure~\ref{fig:pairwise-preferences}.}
    \label{fig:oneshot-prompt}
\end{figure}

So far, the transfer pipeline has selected a subset of already judged documents from the source corpus for each query. These documents were then segmented into passages, and each passage was scored to estimate its relevance for later processing steps. Subsequently, three approaches for selecting candidates for each query of the retrieval task for pairwise preference inference were introduced. As the output of [Section~\ref{candidate-selection}], each selected candidate document from the target corpus has been segmented into passages. Each of these passages is paired with the top 15 relevant and bottom 5 non-relevant passages from the source corpus. The queries used for the pairwise preferences are predefined by the corresponding retrieval tasks. At this stage, all necessary processing steps have been completed. The candidates, consisting of \texttt{(query, known passage, passage to judge)}, are now ready for processing, as illustrated in Figure~\ref{fig:pairwise-preferences}.
% So far, the transfer pipeline has selected a subset of documents from the source corpus for each query. These documents were then segmented into passages, and each passage was scored to identify its relevance, used for later processing steps. Subsequently, three approaches were described for determining candidates for pairwise preference inference. As an output of [Section~\ref{candidate-selection}], each selected document from the target corpus has been broken down into passages and each passage is paired with 15 relevant and 5 non-relevant passages from the source corpus. The queries used for the pairwise preferences are predefined by the corresponding retrieval tasks. At this stage, all necessary processing steps have been completed to perform pairwise preference inference. These resulting candidates, consisting of \texttt{(query, known passage, passage to judge)}, are now ready for processing, as visualized in Figure~\ref{fig:pairwise-preferences}.
\\\\
To perform the pairwise preference inference, the Python library \texttt{autoqrels}\footnote{https://github.com/seanmacavaney/autoqrels} was used. \texttt{autoqrels} is a tool designed for automatically inferring relevance judgments. It supports zero-shot and one-shot prompting for relevance inference and includes several pre-implemented inference models. The original \texttt{autoqrels} paper~\citep{macavaney:2023} evaluated the effectiveness of one-shot labelers for automatic relevance estimation. Among the tested approaches \texttt{MaxRep-BM25}, \texttt{MaxRep-TCT}, \texttt{DuoT5}, and \texttt{DuoPrompt}, the \texttt{DuoPrompt} approach for pairwise inference demonstrated superior performance and therefore is utilized for one-shot labeling in this thesis.
\\\\
\texttt{DuoPrompt} processes a \texttt{query}, a known passage \texttt{A}, and a passage to judge \texttt{B}. It constructs a prompt that includes a brief instruction for a transformer model, asking the model to assess the relevance of passage \texttt{B} for the given query, as shown in Figure~\ref{fig:oneshot-prompt}. This prompt is then fed into \mbox{\texttt{FLAN-T5}~\citep{chung:2022}}, along with the question of whether passage \texttt{B} is as relevant as passage \texttt{A}.
\\\\
\texttt{FLAN-T5} is a Text-to-Text Transfer Transformer model, an advanced version of Google's \texttt{T5} model~\citep{raffel:2020}. While maintaining the backbone architecture of \texttt{T5}, \texttt{FLAN-T5} has been further fine-tuned on a diverse set of training tasks, enhancing its performance across various natural language processing applications. \texttt{FLAN-T5} is available in multiple sizes, ranging from smaller, resource-efficient versions like \texttt{flan-t5-small}, with 80~million parameters, to models such as \texttt{flan-t5-xxl}, which contains 11~billion parameters. These variations accommodate different computational and application needs. Given the computational constraints for this thesis, \texttt{flan-t5-base}\footnote{https://huggingface.co/google/flan-t5-base} model, with 250~million parameters, was selected as the transformer model for \texttt{DuoPrompt}.
\\\\
Figure~\ref{fig:oneshot-prompt} illustrates the prompt structure provided to \texttt{FLAN-T5}. The prompt takes a query, a known passage (referred to as \texttt{rel\_doc\_text}), and a passage to judge (referred to as \texttt{unk\_doc\_text}) as inputs for pairwise inference. The model then predicts the probability values by computing the softmax over the logits of two tokens, which are subsequently used as the relevance score. If the passage to judge is highly relevant to the query, the model's expected output should be a score close to 1.0. Conversely, if the passage is not relevant at all for the query, the model should produce a score close to 0.0.
\\\\\\\\
All candidates undergo pairwise preference inference, where each passage to judge is compared against 20 known passages, as outlined in Section \ref{composing-final-candidates}. Accordingly, the outcome of the pairwise preference inference generates 20 relevance scores for each passage to judge, with higher scores indicating greater relevance to the query. A final relevance for each passage is then computed by aggregating all of its inferred scores.
\\\\
This chapter provided a comprehensive description of all steps in the transfer pipeline, resulting in a set of relevance scores for each passage to judge. These scores represent the transferred relevance judgments from the source dataset's retrieval task into the target corpus \texttt{ClueWeb22/b}. The next chapter will evaluate the effectiveness of the individual steps and the overall quality of the transferred relevance judgments across datasets.
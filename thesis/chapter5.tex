\chapter{Conclusion \& Further Work}\label{conclusion}

In this thesis, I presented an approach for automatically generating new relevance judgments by leveraging existing judgments from an annotated dataset. The source dataset consisted of a document corpus and a retrieval task that provided queries and corresponding relevance judgments. To facilitate the automatic transfer of relevance judgments from the source dataset to a new document corpus, a transfer pipeline was developed. The evaluation of the pipeline in a self-transfer setting demonstrated that the pairwise approach significantly outperformed the pointwise approach in generating relevance judgments. \colorbox{red}{Waiting for Maik, pipeline to ClueWeb22/b}. This confirms that the relevance transfer process improves the quality of automatically generated relevance judgments. A  requirement for a successful transfer is that the source dataset must contain a sufficient number of high-quality relevance judgments.
\\\\
The comparison of different versions of the \texttt{T5} model indicated that more fine-tuned models achieved higher rank correlations. It is therefore likely that using an even larger version of \texttt{FLAN-T5} with more parameters would achieve even better relevance predictions. Future work could also explore benchmarking and comparing entirely different large language models for performing pairwise preference inference. Another key component of the transfer pipeline was the \texttt{Union opd.\ 100} approach, which was found to be the most effective candidate selection method in this thesis. However, further research could refine the process of selecting likely relevant candidate documents. The nearest neighbor approach already demonstrated strong performance, and combining it with the naive selection approach even increased recall, but at the cost of a larger candidate set. A more fine-tuned version of this approach could be developed to further optimize relevance transfer.


% \\\\
% The pipeline involved several key steps: preprocessing the source dataset, identifying candidate documents from the target corpus, and generating new relevance judgments. For each query in the retrieval task, selected candidate documents were segmented into passages. Each passage was then compared against the 15 most relevant and 5 least relevant passages from the source dataset for the same query. A large language model performed pairwise preference comparisons to assess how relevant each target passage was relative to the known source passage. These comparisons were then aggregated to assign a final relevance score to each target document.
\section*{Abstract}

Relevance judgments are essential for evaluating and comparing the effectiveness of information retrieval systems. Traditionally, human assessors manually review query-document pairs to determine relevance judgments. This process is costly, time-consuming, and must be repeated for each new test collection. This thesis addresses this challenge by presenting a method for automatically generating relevance judgments using existing annotated datasets. The proposed approach transfers relevance information from a well-judged source corpus to a subset of documents in a target corpus, enriching it with newly generated judgments. The method employs a pairwise preference approach, where a large language model compares already judged documents from the source corpus with candidate documents from the target corpus. To do this, the model is prompted to determine whether a target document is as relevant to a given query as an already judged source document, resulting in an automatically generated set of relevance judgments for the target corpus. To evaluate the effectiveness of the developed approach, the transfer method is applied to multiple existing test collections that already contain relevance judgments. Each collection serves as both a source and its own target corpus, enabling automatic evaluation by comparing the newly generated judgments with the original ones. Additionally, the approach is tested on \texttt{ClueWeb22/b} as an unjudged target corpus. By leveraging pairwise preference with already judged documents, this approach has the potential to significantly reduce the effort for manual annotation while maintaining high-quality relevance judgments, making scalable enrichment of target corpora possible.

% Relevance judgments are essential for evaluating and comparing the effectiveness of information retrieval systems. Traditionally, human assessors determine relevance by reviewing query-document pairs, but this process is costly, time-consuming, and must be repeated for each new test collection. This thesis presents a novel approach to automatically generating relevance judgments by leveraging existing annotated datasets. The core idea is to transfer relevance information from a well-judged source corpus to an unlabeled target corpus. To achieve this, documents from the source corpus—already judged for specific queries—are compared to documents from the target corpus. Using pairwise preference evaluations, a large language model determines whether a target document is as relevant to a given query as a judged source document. The outcome is an inferred set of relevance judgments for a subset of the target corpus. This method has the potential to drastically reduce human annotation effort while maintaining high-quality relevance assessments, enabling scalable and automated evaluation of information retrieval systems.
% \\\\
% To evaluate the effectiveness of this transfer approach, the process is applied to multiple test collections. Each collection serves both as a source with pre-existing relevance judgments and as its own target corpus. This allows for automatic evaluation by comparing the newly generated judgments with the original ones. Additionally, the transfer process is tested with the \colorbox{red}{recently released } \texttt{ClueWeb22} corpus, containing over 1.0 billion documents but limited existing relevance judgments. A successful transfer enriches \texttt{ClueWeb22} with valuable judgments, enhancing its utility for scalable and automated evaluation of information retrieval systems.

\pagebreak

\section*{Kurzfassung}

\colorbox{red}{UEBERSETZUNG DES ABSTRACTS INS DEUTSCHE}\\
Relevance judgments are essential for evaluating and comparing the effectiveness of information retrieval systems. Traditionally, human assessors manually review query-document pairs to determine relevance judgments. This process is costly, time-consuming, and must be repeated for each new test collection. This thesis addresses this challenge by presenting a method for automatically generating relevance judgments using existing annotated datasets. The proposed approach transfers relevance information from a well-judged source corpus to a subset of documents in a target corpus, enriching it with newly generated judgments. The method employs a pairwise preference approach, where a large language model compares already judged documents from the source corpus with candidate documents from the target corpus. To do this, the model is prompted to determine whether a target document is as relevant to a given query as an already judged source document, resulting in an automatically generated set of relevance judgments for the target corpus. To evaluate the effectiveness of the developed approach, the transfer method is applied to multiple existing test collections that already contain relevance judgments. Each collection serves as both a source and its own target corpus, enabling automatic evaluation by comparing the newly generated judgments with the original ones. Additionally, the approach is tested on \texttt{ClueWeb22/b} as an unjudged target corpus. By leveraging pairwise preference with already judged documents, this approach has the potential to significantly reduce the effort for manual annotation while maintaining high-quality relevance judgments, making scalable enrichment of target corpora possible.
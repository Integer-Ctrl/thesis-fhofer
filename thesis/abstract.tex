Relevance judgments are essential for evaluating and comparing the effectiveness of information retrieval systems. Traditionally, human assessors determine relevance by reviewing query-document pairs, but this process is costly, time-consuming, and must be repeated for each new test collection. This thesis presents a novel approach to automatically generating relevance judgments by leveraging existing annotated datasets. The core idea is to transfer relevance information from a well-judged source corpus to an unlabeled target corpus. To achieve this, documents from the source corpus—already judged for specific queries—are compared to documents from the target corpus. Using pairwise preference evaluations, a large language model determines whether a target document is as relevant to a given query as a judged source document. The outcome is an inferred set of relevance judgments for a subset of the target corpus. This method has the potential to drastically reduce human annotation effort while maintaining high-quality relevance assessments, enabling scalable and automated evaluation of information retrieval systems.
\\\\
To evaluate the effectiveness of this transfer approach, the process is applied to multiple test collections. Each collection serves both as a source with pre-existing relevance judgments and as its own target corpus. This allows for automatic evaluation by comparing the newly generated judgments with the original ones. Additionally, the transfer process is tested with the \colorbox{red}{recently released } \texttt{ClueWeb22} corpus, containing over 1.0 billion documents but limited existing relevance judgments. A successful transfer enriches \texttt{ClueWeb22} with valuable judgments, enhancing its utility for scalable and automated evaluation of information retrieval systems.

\section*{Abstract}

Relevance judgments are essential for evaluating and comparing the effectiveness of information retrieval systems. Traditionally, human assessors manually review query-document pairs to determine relevance judgments. This process is costly, time-consuming, and must be repeated for each new test collection. This thesis addresses this challenge by presenting a method for automatically generating relevance judgments using existing annotated datasets. The proposed approach transfers relevance information from a well-judged source corpus to a subset of documents in a target corpus, enriching it with newly generated judgments. The method employs a pairwise preference approach, where a large language model compares already judged documents from the source corpus with candidate documents from the target corpus. To do this, the model is prompted to determine whether a target document is as relevant to a given query as an already judged source document, resulting in an automatically generated set of relevance judgments for the target corpus. To evaluate the effectiveness of the developed approach, the transfer method is applied to multiple existing test collections that already contain relevance judgments. Each collection serves as both a source and its own target corpus, enabling automatic evaluation by comparing the newly generated judgments with the original ones. Additionally, the approach is tested on \texttt{ClueWeb22/b} as an unjudged target corpus. By leveraging pairwise preference with already judged documents, this approach has the potential to significantly reduce the effort for manual annotation while maintaining high-quality relevance judgments, making scalable enrichment of target corpora possible.

\pagebreak

\section*{Kurzfassung}

Relevanzbewertungen sind für die Evaluierung und den Vergleich der Effektivität von Information Retrieval Systemen von entscheidender Bedeutung. Traditionell werden diese Bewertungen von menschlichen Annotatoren manuell durch die Bewertung von Anfrage-Dokument-Paaren bestimmt. Dieser Prozess ist jedoch zeitaufwändig und muss für jeden neuen Testdatensatz wiederholt werden. Diese Arbeit untersucht, ob der Aufwand für die Erstellung von Relevanzbewertungen reduziert werden kann, indem bestehende annotierte Datensätze zur automatischen Generierung neuer Bewertungen verwendet werden. Dazu wird ein Ansatz vorgestellt, der Relevanzinformationen aus einem gut annotierten Datensatz extrahiert und auf einen neuen Datensatz überträgt. Das Verfahren basiert auf paarweisen Vergleichen. Dazu werden bereits annotierte Dokumente eines Quelldatensatzes mit Dokumenten eines Zieldatensatzes in einem Large Language Model verglichen. Das Modell hat die Aufgabe zu bestimmen, ob ein Zieldokument für eine bestimmte Anfrage genauso relevant ist wie ein bereits bewertetes Quelldokument. Das Ergebnis ist eine automatisch generierte Menge von Relevanzbewertungen für den Zieldatensatz. Zur Evaluierung des Ansatzes wird dieser auf mehrere bestehende Testdatensätze angewendet, die bereits Relevanzbewertungen enthalten. Jede dieser Sammlungen dient sowohl als Quell- als auch als Zieldatensatz, was einen Vergleich der generierten Relevanzbewertungen mit den ursprünglichen Relevanzbewertungen ermöglicht. Zusätzlich wird der Transfer mit \texttt{ClueWeb22/b} als Zieldatensatz getestet. Durch die Nutzung bereits annotierter Dokumente und paarweiser Vergleiche hat dieser Ansatz das Potenzial, den manuellen Annotationsaufwand neuer Datensätze deutlich zu reduzieren und gleichzeitig qualitativ hochwertige Relevanzbewertungen zu generieren.
\chapter{Introduction}\label{introduction} 

Static test collections, which consist of information needs, documents, and relevance judgments, are commonly used to evaluate the retrieval effectiveness of retrieval systems \cite{sanderson:2010}. Retrieval effectiveness measures how well a retrieval system retrieves relevant documents to a given information need. To assess this effectiveness, relevance judgments are required to classify the retrieved results as relevant or non-relevant to an information need. However, the use of such static test collections relies on several simplications that are not realistic in practice. One assumption is that information needs, documents, and relevance judgments remain unchanged over time. While information needs are often stable, documents frequently change over time \mbox{\cite{cho:2000}}. This presents a challenge for evaluation methods based on static collections, as relevance judgments made for one version of a document may not apply to later versions.
\\\\
Relevance judgments are assessments made by humans as to whether a document is relevant to a given query. This process is time-consuming and expensive due to human needs. One of the first widely used test collections, the Cranfield test collection \mbox{\cite{cleverdon:91}}, included a small corpus of just $1\,400$ scientific abstracts with 225 queries and manually assigned relevance judgments. While feasible for small collections, judging with human effort becomes impractical when dealing with modern large-scale corpora containing millions or even billions of documents. Due to the large amount of data, only a small subset of documents can be judged anyway. 
\\\\\\\\\\\\\\
In this thesis, I propose an approach for transferring relevance judgments from an existing test collection to a target corpus using pairwise preferences. The goal is to automate the creation of relevance judgments for the target corpus, thereby enriching its set of relevance judgments and enabling more accurate evaluation of retrieval systems on the target corpus. Additionally, this approach aims to reduce the need for human judgment, which would otherwise be \mbox{required} for manually assessing relevance judgments.
\\\\
The transfer pipeline consists of several steps that process the source and target datasets to perform pairwise preference comparisons between documents from both corpora in order to generate new relevance judgments. The pipeline starts with a source dataset that includes a document corpus and an associated retrieval task that provides a set of queries and relevance judgments for the documents in the corpus.
\\\\
Only documents from the source corpus that have at least one relevance judgment contain information that can be used for relevance transfer. Therefore, an initial document selection is performed on the source document corpus. To enable a more fine-granular comparison, the selected documents are segmented into smaller text passages. This segmentation is necessary because later steps, particularly the pairwise preference evaluation, rely on transformer models, which have a maximum context length they can process. Another advantage of segmenting documents is that relevant information is typically concentrated in specific parts of a document rather than being spread throughout. To identify these relevant parts, the resulting passages are ranked to determine the most relevant passages for each query in the retrieval task. The highest-scoring passages are then used in the next step to identify documents from the target corpus that are likely to be relevant to the query and should therefore be judged. The same segmentation approach is applied to the selected candidate documents from the target corpus.
\\\\
To finally conduct the pairwise preference comparisions, each target passage is paired with the most relevant passages from the source dataset for the corresponding query. A large language model is provided with tuples of the selected canidates, consisting of \texttt{(query, known source passage, target passage to judged)}. The model is prompted to determine the relevance of the target passage to the query based on the known passage. The resulting relevance scores from these pairwise comparisons are then used to generate the final relevance judgments for the target documents. After aggregating the relevance scores, the pipeline outputs a set of relevance judgments for each query in the retrieval task for the selected subset of target documents.
\\\\
The final part of this thesis focuses on evaluating the effectiveness of the relevance transfer pipeline. For this purpose, widely used datasets such as \texttt{Args.me}, \texttt{Disks 4+5}, and \texttt{MS MARCO} serve as source corpora. Their existing retrieval tasks provide the foundation for the transfer process, which is evaluated in two phases. First, the pipeline is applied within the source datasets themselves to assess how accurately the transfer process can reproduce known relevance judgments. To compare the generated relevance judgments with the actual ones, rank correlation is computed between both label sets. This allows for an automated evaluation against the existing relevance judgments of the source datasets. In the second phase, the pipeline is used to transfer relevance judgments from the source datasets to \texttt{ClueWeb22/b}, the selected target corpus. Since this corpus lacks pre-existing relevance judgments, evaluation in this case requires manual assessment. To address this, a representative subset of relevance judgments will be manually assessed for \texttt{ClueWeb22/b} in order to evaluate the quality of the automatically created ones.
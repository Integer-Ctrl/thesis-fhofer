\chapter{Introduction}\label{introduction} 

Static test collections, which consist of information needs, documents, and relevance judgments, are commonly used to evaluate the retrieval effectiveness of retrieval systems \citep{sanderson:2010}. Retrieval effectiveness measures how well a retrieval system retrieves relevant documents to a given information need. To assess this effectiveness, relevance judgments are required to classify the retrieved results as relevant or non-relevant to an information need. However, the use of such static test collections relies on several simplications that are not realistic in practice. One assumption is that information needs, documents, and relevance judgments remain unchanged over time. While information needs are often stable, documents frequently change over time \citep{cho:2000}. This presents a challenge for evaluation methods based on static collections, as relevance judgments made for one version of a document may not apply to later versions.
\\\\
Relevance judgments are judgments made by humans as to whether a document is relevant to a given query. This relevance assessment process is time-consuming and expensive due to human needs. The Cranfield collection \citep{cleverdon:91}, one of the first widely used test collections, consisted of a small corpus of just 1,400 scientific abstracts with 225 queries and manually assigned relevance judgments. While feasible for small collections, assessing with human effort becomes impractical when dealing with modern large-scale corpora containing millions or even billions of documents. Due to the large amount of data, only a small subset of documents can be judged anyway. 
\\\\
In this thesis, I propose an approach for transferring relevance judgments from an existing test collection to a target corpus without relevance judgments using pairwise preferences. The goal is to automate the creation of relevance judgments for the target corpus, enabling its evaluation or enriching the number of relevance judgments, thereby allowing for more accurate evaluation results. Additionally, this approach aims to reduce the need for human labor, which would otherwise be required to manually assess relevance for the target corpus. At the end of the transfer pipeline, the goal is to generate a set of relevance assessments for a selection of documents from the target corpus, ensuring that each query from the original retrieval task is covered.
\\\\
The transfer pipeline consists of several steps, which will be explained in detail later. First, documents from the source dataset of the retrieval task that already have relevance assessments are segmented into smaller text passages. This segmentation is necessary because later steps, particularly the pairwise preference evaluation at the end, rely on transformer models. Those LLMs have common limitations like a maximum context lenght to work effectively. To comply with this limitation the documents are segmented into passages. The resulting passages from the source dataset are then scored to identify the most relevant passages for each query in the retrieval task. The highest-scoring passages serve as known relevant passages for the final pairwise preference step and help determine which documents from the target corpus should be assessed. As mentioned before, due to the use of LLMs for pairwise preference evaluation, the selected documents are also segmented into passages. In the final step, pairwise preferences are computed between the top known relevant passages and the selected passages from the target dataset.
\\\\
The next part of this thesis focuses on evaluating the transfer pipeline. To achieve this, widely used datasets such as \texttt{Args.me}, \texttt{Disks 4+5}, and \texttt{MS MARCO} serve as the source datasets, along with their existing retrieval tasks and relevance judgments. The evaluation is conducted in two directions. First, the pipeline is applied to transfer relevance judgments within the source dataset itself, assessing how well the approach reproduces known relevance assessments. Second, the pipeline is applied to \texttt{ClueWeb22/b}, chosen as the target corpus due to be the most recent ClueWeb collection and its sparse relevance judgments. The evaluation of the self-transfer process is performed using rank correlation metrics, allowing for an automated assessment against the source datasets existing relevance judgments. In contrast, the evaluation of the transfer to \texttt{ClueWeb22/b} requires manual assessment, as no pre-existing relevance judgments are available. A subset of the generated relevance judgments will therefore be manually reviewed to evaluate their quality.
\chapter{Introduction}\label{introduction} 

Static test collections, which consist of information needs, documents, and relevance judgments, are commonly used to evaluate the retrieval effectiveness of retrieval systems \citep{sanderson:2010}. Retrieval effectiveness measures how well a retrieval system retrieves relevant documents to a given information need. To assess this effectiveness, relevance judgments are required to classify the retrieved results as relevant or non-relevant to an information need. However, the use of such static test collections relies on several simplications that are not realistic in practice. One assumption is that information needs, documents, and relevance judgments remain unchanged over time. While information needs are often stable, documents frequently change over time \mbox{\citep{cho:2000}}. This presents a challenge for evaluation methods based on static collections, as relevance judgments made for one version of a document may not apply to later versions.
\\\\
Relevance judgments are judgments made by humans as to whether a document is relevant to a given query. This relevance judgment process is time-consuming and expensive due to human needs. The Cranfield collection \citep{cleverdon:91}, one of the first widely used test collections, consisted of a small corpus of just $1\,400$ scientific abstracts with 225 queries and manually assigned relevance judgments. While feasible for small collections, judging with human effort becomes impractical when dealing with modern large-scale corpora containing millions or even billions of documents. Due to the large amount of data, only a small subset of documents can be judged anyway. 
\\\\\\\\\\\\
% In this thesis, I propose an approach for transferring relevance judgments from an existing test collection to a target corpus using pairwise preferences. The goal is to automate the creation of relevance judgments for the target corpus, enriching the number of relevance judgments, thereby allowing for more \mbox{accurate} evaluation of retrieval systems on the target corpus. Additionally, this approach aims to reduce the need for human labor, which would otherwise be required to manually assess relevance judgments. At the end of the transfer pipeline, the goal is to generate a set of relevance judgments for a selection of documents from the target corpus, ensuring that each query from the original retrieval task is covered.
In this thesis, I propose an approach for transferring relevance judgments from an existing test collection to a target corpus using pairwise preferences. The goal is to automate the creation of relevance judgments for the target corpus, thereby enriching its set of relevance judgments and enabling more accurate evaluation of retrieval systems on the target corpus. Additionally, this approach aims to reduce the need for human judgment, which would otherwise be \mbox{required} for manually assessing relevance judgments.
\\\\
The transfer pipeline consists of several steps, processing the source and target datasets to conduct pairwise preference comparisons between source and target corpora documents, generating new relevance judgments. The pipeline begins with a source dataset that includes a document corpus and an associated retrieval task, which provides a set of queries and relevance judgments for the documents in the corpus.
\\\\
Only documents from the source corpus that have at least one relevance judgment contain information that can be used for relevance transfer. Therefore, an initial document selection is performed on the source document corpus. To enable a more fine-granular comparison, the selected documents are segmented into smaller text passages. This segmentation is necessary because later steps, particularly the pairwise preference evaluation, rely on transformer models, which have a maximum context length they can process. Another advantage of segmenting documents is that relevant information is typically concentrated in specific parts of a document rather than being spread throughout. To identify these relevant parts, the resulting passages are ranked to determine the most relevant passages for each query in the retrieval task. The highest-scoring passages are then used in the next step to identify documents from the target corpus that are likely to be relevant to the query and should therefore be judged. The same segmentation approach is applied to the selected candidate documents from the target corpus.
% Initially, documents from the source dataset that already have relevance judgments are segmented into passages. This segmentation is necessary because later steps, particularly the pairwise preference evaluation, rely on transformer models, which have a maximum context length they can process. To enable pairwise preference, the comparisons works at passage level, complying with this input lenght limitation. Another advantage of segmenting documents is that relevant information is typically concentrated in specific parts of a document rather than being spread throughout. To identify these relevant parts, the resulting passages are scored to determine the most relevant passages for each query in the retrieval task. The highest-scoring passages are then used to identify documents from the target corpus that are likely to be relevant to the query and should therefore be judged. The same segmentation approach is applied to the selected candidate documents from the target corpus.
\\\\
To finally conduct the pairwise preference comparisions, each target passage is paired with the most relevant passages from the source dataset for the corresponding query. A large language model is provided with tuples of the selected canidates, consisting of \texttt{(known source passage, target passage to judged, query)}. The model is prompted to determine the relevance of the target passage to the query based on the known passage. The resulting relevance scores from these pairwise comparisons are then used to generate the final relevance judgments for the target documents. After aggregating the relevance scores, the pipeline outputs a set of relevance judgments for each query in the retrieval task for the selected subset of target documents.
% At this point, the pipeline selected a subset of target documents for each query in the retrieval task. These documents are represented as passages, and each target passage is paired with the most relevant passages from the source dataset for the corresponding query. Now, the pairwise preference comparisons can take place. A large language model is provided with tuples of the selected canidates, consisting of \texttt{(known source passage, target passage to be judged, query)}. The model is prompted to determine the relevance of the target passage to the query based on the known passage. The resulting relevance scores from these pairwise comparisons are then used to generate the final relevance judgments for the target documents. After aggregating the relevance scores, the pipeline outputs a set of relevance judgments for each query in the retrieval task for the selected subset of target documents.
% The transfer pipeline consists of several steps, which will be explained in detail later. First, documents from the source dataset of the retrieval task that already have relevance judgments are segmented into smaller text passages. This segmentation is necessary because later steps, particularly the pairwise preference evaluation at the end, rely on transformer models. Those large language models have common limitations like a maximum context lenght to work effectively. To comply with this limitation the documents are segmented into passages. The resulting passages from the source dataset are then scored to identify the most relevant passages for each query in the retrieval task. The highest-scoring passages serve as known relevant passages for the final pairwise preference step and help determine which documents from the target corpus should be assessed. As mentioned before, due to the use of large language models for pairwise preference evaluation, the selected documents are also segmented into passages. In the final step, pairwise preferences are computed between the top known relevant passages and the selected passages from the target dataset.
\\\\
The final part of this thesis focuses on evaluating the effectiveness of the relevance transfer pipeline. For this purpose, widely used datasets such as \texttt{Args.me}, \texttt{Disks 4+5}, and \texttt{MS MARCO} serve as source corpora. Their existing retrieval tasks provide the foundation for the transfer process, which is evaluated in two phases. First, the pipeline is applied within the source datasets themselves to assess how accurately the transfer process can reproduce known relevance judgments. To compare the generated relevance judgments with the actual ones, rank correlation is computed between both label sets. This allows for an automated evaluation against the existing relevance judgments of the source datasets. In the second phase, the pipeline is used to transfer relevance judgments from the source datasets to \texttt{ClueWeb22/b}, the selected target corpus. Since this corpus lacks pre-existing relevance judgments, evaluation in this case requires manual assessment. To address this, a representative subset of relevance judgments will be manually assessed for \texttt{ClueWeb22/b} in order to evaluate the quality of the automatically created ones.
% The final part of this thesis focuses on evaluating the effectiveness of the relevance transfer pipeline. For this purpose, widely used datasets such as \texttt{Args.me}, \texttt{Disks 4+5}, and \texttt{MS MARCO} are used  as source corpora. Their existing retrieval tasks serve as the starting point for the transfer process from where the evaluation is conducted in two phases. First, the pipeline is applied to transfer relevance judgments within the source datasets themselves, assessing how accurately the approach can reproduce known relevance labels. In order to evaluate the created relevance labels to the actual ones, the evaluation is done computing the rank correlation between both label sets ,allowing for an automated judgment against the source datasets existing relevance judgments. Second, the pipeline is applied to transfer relevance judgments from the source datasets to \texttt{ClueWeb22/b}, the selected target corpus. Since no pre-existing relevance judgments are available for this corpus, the evaluation here requires manual assessment. To address this, a representative subset of relevance judgments will be manually assessed in order to evaluate the created judgments.
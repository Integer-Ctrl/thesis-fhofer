\chapter{Introduction}\label{introduction} 

Retrieval effectiveness is one of the most important aspects through which an information retrieval system can be evaluated. Retrieval effectiveness measures how well an information retrieval system retrieves documents that are relevant to a query. To test and compare different information retrieval systems it is necessary to create a test environment with consistent conditions that don't change. To achieve this, evaluation is done on test collections \citep{sanderson:2010}. A test collection is defined by a fixed set of documents, the corpus, a fixed set of queries, also known as topics, and a fixed set of relevance judgments. The complexity of creating a corpus is limited, as this process can be largely automated. The effort required to develop matching topics for the corpus is also low, as mostly only 50 topics matching the domain of the corpus need to be created once and then can be reused. The most complex part is the elaboration of the relevance judgments. To evaluate information retrieval systems' effectiveness in retrieving relevant results regarding a topic, the retrieved results must be classified as whether they are relevant to the topic. Relevance judgments, usually called query relevance labels (\texttt{qrels}) are judgments made by humans as to whether a document is relevant to a topic. A \texttt{qrel} is an tripel of (document, query, relevance). This relevance assessment process is time-consuming and expensive due to human needs.
\\\\
The Cranfiel collection \citep{cleverdon:91} was the first test collection widely used for evaluation. It comprises a small corpus of 1,400 scientific abstracts, 225 queries, and relevance judgments for each query. Relevance judgment on a small corpus like Cranfield is feasible, but today corpora are much larger with millions to billions of documents. Because of the large amount of documents, only a small subset can be judged. When evaluating an information retrieval system, two key measures are precision and recall \citep{voorhees:2001}. Precision is the fraction of retrieved documents relevant to the query and the total number of retrieved documents. Recall is the fraction of relevant documents retrieved by the system and the total number of relevant documents. Since not all query-document pairs are judged, often not all retrieved documents can be classified as relevant or non-relevant. To address this there are three common strategies \citep{froebe:2023}. The first two approaches are to consider all unjudged documents as relevant or as non-relevant. This comes with some drawbacks, e.g., measures like Rank-Biased Precision enable to derive upper and lower bounds on effectiveness, and measures like average precision are unstable when unjudged documents are discovered to be relevant \citep{moffat:08}. Another approach is to remove unjudged documents from the retrieved documents, which is also not ideal as it can lead to a bias in the evaluation and inaccurate results.
\\\\
In this thesis, I will propose an approach to transfer relevance judgments from an already judged source dataset to a target dataset using pairwise preferences. The goals are to automate the creation of relevance judgments for the target dataset, increasing the number of \texttt{qrels}, which allows more accurate evaluation results, and reduce human labor who would otherwise have to perform relevance judgment for the target dataset. To achieve these goals, the source dataset is first passed through a transfer pipeline consisting of five stages. First, all documents in the source dataset that have at least one \texttt{qrel} are chunked into passages. This later enables a more precise selection of the text fragments that are most relevant for the query associated with the \texttt{qrel}. In the second stage, the passages will receive a score that indicates how relevant the passage is to a specific query. Therefore for all \texttt{qrels} in the source dataset, \texttt{(document, query, relevance)} triple, every passage of the document is treated as a query.
\\\\
Using various information retrieval systems, such as BM25, TF-IDF, or DLH, searches are performed against the source dataset index. From the resulting ranking lists, \texttt{precision}@10 and \texttt{nDCG@10} are calculated and used as scores for each passage. \texttt{precision} and \texttt{nDCG} are computed for every passage and for each system. To determine which system-metric combination best represents the original relevance labels, a ranking of the systems and metrics is done in the third stage. Therefore, the rank correlations for all system-metric combinations and the original relevance labels from the \texttt{qrels} are computed. In the fourth stage, the candidate retrieval, documents from the target dataset are identified for which relevance judgments will be created in the final step. Using a nearest-neighbor search, documents from the target dataset that are most similar to those in the source dataset are selected. These documents are also chunked into passages. Each candidate passage will be compared to the top 20 passages from the source dataset that are most relevant to the query for which the relevance judgment is made. Pairwise preferences are then calculated for each query, consisting of the target dataset passage and the 20 selected passages from the source dataset. To finally get the resulting relevance judgments for the candidates, the pairwise preferences are aggregated, first at the passage level and then at the document level. The resulting relevance judgments are then evaluated on the target dataset.
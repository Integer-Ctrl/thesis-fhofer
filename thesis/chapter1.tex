\chapter{Introduction}\label{introduction} 

Static test collections, which consist of information needs, documents, and relevance judgments, are commonly used to evaluate the retrieval effectiveness of retrieval systems \citep{sanderson:2010}. Retrieval effectiveness measures how well a retrieval system retrieves relevant documents to a given information need. To assess this effectiveness, relevance judgments are required to classify the retrieved results as relevant or non-relevant to an information need. However, the use of such static test collections relies on several simplications that are not realistic in practice. One assumption is that information needs, documents, and relevance judgments remain unchanged over time. While information needs are often stable, documents frequently change over time \citep{cho:2000}. This presents a challenge for evaluation methods based on static collections, as relevance judgments made for one version of a document may not apply to later versions.
\\\\
Relevance judgments are judgments made by humans as to whether a document is relevant to a given query. This relevance judgment process is time-consuming and expensive due to human needs. The Cranfield collection \citep{cleverdon:91}, one of the first widely used test collections, consisted of a small corpus of just 1,400 scientific abstracts with 225 queries and manually assigned relevance judgments. While feasible for small collections, judging with human effort becomes impractical when dealing with modern large-scale corpora containing millions or even billions of documents. Due to the large amount of data, only a small subset of documents can be judged anyway. 
\\\\
In this thesis, I propose an approach for transferring relevance judgments from an existing test collection to a target corpus without relevance judgments using pairwise preferences. The goal is to automate the creation of relevance judgments for the target corpus, enabling its evaluation or enriching the number of relevance judgments, thereby allowing for more accurate evaluation results. Additionally, this approach aims to reduce the need for human labor, which would otherwise be required to manually assess relevance for the target corpus. At the end of the transfer pipeline, the goal is to generate a set of relevance judgments for a selection of documents from the target corpus, ensuring that each query from the original retrieval task is covered.
\\\\
The transfer pipeline consists of several steps, which will be explained in detail later. First, documents from the source dataset of the retrieval task that already have relevance judgments are segmented into smaller text passages. This segmentation is necessary because later steps, particularly the pairwise preference evaluation at the end, rely on transformer models. Those large language models have common limitations like a maximum context lenght to work effectively. To comply with this limitation the documents are segmented into passages. The resulting passages from the source dataset are then scored to identify the most relevant passages for each query in the retrieval task. The highest-scoring passages serve as known relevant passages for the final pairwise preference step and help determine which documents from the target corpus should be assessed. As mentioned before, due to the use of large language models for pairwise preference evaluation, the selected documents are also segmented into passages. In the final step, pairwise preferences are computed between the top known relevant passages and the selected passages from the target dataset.
\\\\
The final part of this thesis focuses on evaluating the effectiveness of the relevance transfer pipeline. For this purpose, widely used datasets such as\texttt{Args.me}, \texttt{Disks 4+5}, and \texttt{MS MARCO} are used  as source datasets. Their existing retrieval tasks and relevance judgments serve as the starting point for the transfer process. The evaluation is conducted in two phases. First, the pipeline is applied to transfer relevance judgments within the source datasets themselves, assessing how accurately the approach can reproduce known relevance labels. This self-transfer evaluation uses rank correlation metrics,allowing for an automated judgment against the source datasets existing relevance judgments. Second, the pipeline is applied to transfer relevance judgments from the source datasets to \texttt{ClueWeb22/b}, the selected target corpus. Since no pre-existing relevance judgments are available for this corpus, the evaluation here requires manual assessment. To address this, a representative subset of relevance judgments will be manually assessed in order to evaluate the inferred judgments.
% The last part of this thesis focuses on evaluating the transfer pipeline. To achieve this, widely used datasets such as \texttt{Args.me}, \texttt{Disks 4+5}, and \texttt{MS MARCO} are used as source datasets, along with their existing retrieval tasks and relevance judgments serve as the starting point for the transfer pipeline. The evaluation is conducted in two directions. First, the pipeline is applied to transfer relevance judgments within the source datasets themselfes, assessing how well the approach reproduces known relevance judgments. Second, the pipeline is applied to transfer from the source datasets into \texttt{ClueWeb22/b}, chosen as the target corpus. The evaluation of the self-transfer process is performed using rank correlation metrics, allowing for an automated judgment against the source datasets existing relevance judgments. In contrast, the evaluation of the transfer into \texttt{ClueWeb22/b} requires manual judgments, as no pre-existing relevance judgments are available. A subset of the generated relevance judgments will therefore be manually reviewed to evaluate their quality.
\chapter{Evaluation}\label{evaluation}

This chapter evaluates the developed relevance transfer pipeline. First, the \mbox{individual} steps of the pipeline are analyzed to examine the intermediate results and identify potential weaknesses, including document selection, passage scoring, and candidate selection. Document selection and passage scoring are performed solely on the source datasets and do not depend on the target dataset. However, to assess the quality of the candidate selection approaches, the transfer pipeline is applied to the same dataset as both the starting and target point of the relevance transfer. Evaluating candidate retrieval requires a target dataset with existing relevance judgments for the queries of the source dataset. Since candidate retrieval aims to pre-select documents from a target corpus that are likely relevant to a query, these judgments are necessary to determine how many selected documents are truly relevant.
\\\\
The second part of the chapter evaluates the inferred relevance judgments through two transfer strategies. The first evaluation applies the transfer pipeline to a source dataset and uses the same dataset as the target. This self-transfer evaluation measures how well the pipeline reproduces known relevance judgments within the same dataset. The second evaluation assesses the final transfer to \texttt{ClueWeb22/b} as the target dataset. Since \texttt{ClueWeb22/b} does not provide relevance judgments for the tested source retrieval tasks, a pooling process is conducted on the candidate documents retrieved by a candidate selection approach. This is followed by a manual relevance judgment of the pooled documents. The quality of the transferred relevance judgments to \texttt{ClueWeb22/b} is then evaluated based on these manual judgments.


% ================
% Rank Correlation
% ================
\section{Rank Correlation}\label{rank-correlation}

\begin{table}[t]
  \centering
  \caption{Comparison of rank correlation metrics computed by Kendall's $\tau$ and Spearman's $\rho$. The table shows the rank correlation between the reference scores $[0.2,0.7,0.5]$ and three label sets: two with an expected correlation of $1$, and one with lower correlation. \texttt{Default} represents the rank correlation scores computed by the standard algorithms, while \texttt{Greedy} shows the scores obtained by greedily mapping the reference scores to the label set, representing the idealized rank correlation outcome.}
  \label{tab:rank-correlation}
  \begin{tabular}{ccccc}
      \toprule
      \textbf{Comparative Set} & \multicolumn{2}{c}{\textbf{Default}} & \multicolumn{2}{c}{\textbf{Greedy}} \\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                               & $\tau$ & $\rho$ & $\tau$ & $\rho$ \\
      \midrule
      
      $[0, 2, 1]$ & 1.00 & 0.99  & 1.00  & 1.00 \\
      $[0, 1, 1]$ & 0.82 & 0.92  & 1.00  & 1.00 \\
      $[0, 0, 1]$ & 0.00 & 0.11  & 0.50  & 0.50 \\
      \bottomrule
  \end{tabular}
\end{table}

Before starting with the evaluation of the transfer pipeline, I present the metrics used to evaluate the relevance judgments produced by the pipeline against the actual relevance judgments provided by the retrieval tasks. This clarification is important, as the evaluation of the assigned passage scores and the final relevance judgments generated by the pipeline are based on these metrics.
\\\\
The \texttt{Inter-Annotator Agreement}~\cite{artstein:2017} is a statistical measure that quantifies the consistency among multiple annotators when labeling the same dataset. It provides insight into annotation quality by indicating the level of agreement or disagreement over all annotations. However, this measure requires all annotators to use the same categorical label set. In this thesis, retrieval tasks assign integer relevance labels ($\in \mathbb{N}$), while the inferred relevance scores are continuous scores ($\in \mathbb{R}^{+}$). Due to this mismatch, the \mbox{\texttt{Inter-Annotator Agreement}} is unsuitable for evaluation.
\\\\
Instead, \texttt{Rank Correlation} is used to compare the actual relevance labels with the inferred relevance scores. \texttt{Rank Correlation} evaluates how well two rankings align. Here, the retrieval task's relevance judgments serve as the ground truth, against which the transfer pipeline's inferred judgments are compared. The rank correlation metrics used are \texttt{Kendall's $\tau$} and \texttt{Spearman's $\rho$}. Both metrics compute the rank correlation between linear orders~\cite{monjardet:1998}, making them well suited for the retrieval task labels and the pipeline's inferred scores. A high rank correlation indicates that the generated judgments preserve the ranking order of the original relevance labels, which is the goal.
\\\\
Table~\ref{tab:rank-correlation} illustrates how these metrics assess some sample relevance judgments. It presents correlation between example reference scores $[0.2, 0.7, 0.5]$ and three label sets: the first two yield an expected correlation of $1$, while the third is expected to have a lower correlation. The \texttt{Default} column contains correlation scores computed by standard \texttt{Kendall's $\tau$} and \texttt{Spearman's $\rho$} algorithms. Unlike the default algorithms, the goal in this thesis is to maximize alignment between inferred relevance scores and ground truth labels. For instance, the relevance scores $[0.2, 0.7, 0.5]$ should ideally correspond to the label set $[0, 1, 1]$ in the second row of Table~\ref{tab:rank-correlation}. This alignment can be achieved by mapping scores less than or equal to $0.2$ to label $0$ and scores greater than $0.2$ to label $1$. Therefore, the intended rank correlation for this example \mbox{should be $1$.}
\\\\
To accomplish this, a modified version called \texttt{Greedy} was tested alongside the default algorithms. This variant greedily maps the relevance scores to the label set, producing the highest possible correlation. The example demonstrates that the standard correlation metrics are sensitive to the rank order of relevance judgments, whereas the \texttt{Greedy} version eliminates this sensitivity, achieving the maximum correlation scores.


% ==================
% Document Selection
% ==================
\section{Document Selection}\label{eval-document-selection}

The first step in the transfer pipeline was selecting and segmenting a subset of documents for each query in a retrieval task from the source corpus. These selected documents play a crucial role in two subsequent steps, identifying candidate documents from the target corpus and performing pairwise preference comparisons using \texttt{DouPrompt}. Since only a subset of documents is required per query, two selection criteria were applied. First, only documents with at least one relevance judgment in the \texttt{qrel store} were considered, as documents without judgments do not provide transferable information. Second, a maximum of 50 relevance judgments per relevance label per query, referred to as a query-label combination, was selected to manage computational complexity.
\\\\
Table~\ref{tab:document-selection} presents the results of the selection process. Based on the selection criteria, a maximum of 50 documents per query-label combination is possible. Ideally, each query should have around 50 non-relevant documents and at least 50 relevant ones. All \texttt{TREC} retrieval tasks performed well, reaching the maximum number of non-relevant documents per query. \texttt{TREC-7} and \texttt{TREC-8} also showed strong results with an average over 40 relevant documents per query. \texttt{Robust04}, despite having a large number of relevance judgments for \makebox[\textwidth][l]{its 250 queries, does not reach an average of 50 relevant documents per query.}
\\\\
This is because only $5.6\%$~\cite{voorhees:1996} of its judgments are relevant judgments. Nonetheless, it maintains a solid average of over 36 relevant documents per query for further processing. Additionally, \texttt{TREC-19 DL} and \texttt{TREC-20 DL} exceeded the threshold of 50 relevant documents per query, ensuring a diverse pool for candidate retrieval and pairwise comparisons. The only outlier is \texttt{Touché 20}, which achieves an average of just 28 non-relevant and 19 relevant judgments per query. The low number of relevance judgments in this retrieval task could limit the effectiveness of the nearest neighbor candidate retrieval approach and the pairwise preference comparisons. However, given the relatively small size of \texttt{Args.me} with 0.4~million documents, the pipeline's \mbox{results may still be valuable}.
\\\\
This intermediate evaluation confirms that the selection process successfully provides an adequate number of documents for subsequent steps in the pipeline. While \texttt{Touché 20} remains useful, its results should be interpreted cautiously due to the lower number of judged documents per query.

\begin{table}[t]
  \centering
  \caption{The table presents the results of the candidate selection process, showing for each label of a retrieval task the number of documents per query for each source dataset. A maximum of 50 documents per query-label combination is possible.}
  \label{tab:document-selection}
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{lcccccc}
      \toprule
      \textbf{Docs./Query} & \textbf{Touché 20} & \textbf{Robust04} & \textbf{TREC-7} & \textbf{TREC-8} & \textbf{TREC-19 DL} & \textbf{TREC-20 DL}  \\

      \midrule

      Label 0        & $27.9$ & $50.0$ & $50.0$ & $50.0$ & $ 49.6$ & $ 50.0$ \\
      Label 1        & $ 6.0$ & $32.8$ & $40.2$ & $40.5$ & $ 31.1$ & $ 28.5$ \\
      Label 2        & $13.0$ & $ 3.8$ &    -   &    -   & $ 23.8$ & $ 16.0$ \\
      Label 3        &    -   &    -   &    -   &    -   & $ 11.7$ & $ 10.3$ \\
      \midrule
      \textbf{Total} & $46.9$ & $86.6$ & $90.2$ & $90.5$ & $116.2$ & $104.8$ \\

      \bottomrule
  \end{tabular}}
\end{table}

\pagebreak

% ===============
% Passage Scoring
% ===============
\section{Passage Scoring}\label{eval-passage-scoring}

\begin{table}[t]
    \centering
    \caption{Average rank correlations between assigned passage scores and original relevance judgments across all queries of a retrieval task, reported using standard Kendall's $\tau$ and Spearman's $\rho$.}
    \label{tab:passage-scoring}
    \resizebox{\textwidth}{!}{%
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{clcccccccccccccc}
        \toprule
        \multicolumn{2}{c}{\textbf{Retrieval Model}} & \multicolumn{2}{c}{\textbf{Touché 20}} & \multicolumn{2}{c}{\textbf{Robust04}} & \multicolumn{2}{c}{\textbf{TREC-7}} & \multicolumn{2}{c}{\textbf{TREC-8}} & \multicolumn{2}{c}{\textbf{TREC-19 DL}} & \multicolumn{2}{c}{\textbf{TREC-20 DL}} & \multicolumn{2}{c}{\textbf{Avg.}} \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12} \cmidrule(lr){13-14} \cmidrule(lr){15-16}
                                                   & & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ \\
        \midrule
        \multirow{10}{*}{\rotatebox{90}{\texttt{ndcg@10}}}
            & BM25         & 0.802 & 0.871 & 0.823 & 0.907 & 0.801 & 0.896 & 0.813 & 0.906 & 0.725 & 0.831 & 0.755 & 0.846 & 0.787 & 0.876 \\
            & DFR\_BM25    & 0.804 & 0.874 & 0.822 & 0.907 & 0.800 & 0.896 & 0.813 & 0.906 & 0.727 & 0.833 & 0.755 & 0.846 & 0.787 & 0.877 \\
            & DFIZ         & 0.778 & 0.852 & 0.820 & 0.906 & 0.796 & 0.896 & 0.800 & 0.898 & 0.730 & 0.836 & 0.753 & 0.845 & 0.780 & 0.872 \\
            & DLH          & 0.839 & 0.901 & 0.831 & 0.913 & 0.810 & 0.903 & 0.825 & 0.915 & 0.735 & 0.839 & 0.754 & 0.845 & 0.799 & 0.886 \\
            & DPH          & 0.783 & 0.856 & 0.820 & 0.905 & 0.799 & 0.895 & 0.809 & 0.903 & 0.725 & 0.831 & 0.753 & 0.844 & 0.782 & 0.872 \\
            & DirichletLM  & 0.679 & 0.752 & 0.816 & 0.902 & 0.798 & 0.896 & 0.802 & 0.898 & 0.721 & 0.825 & 0.751 & 0.841 & 0.761 & 0.852 \\
            & Hiemstra\_LM & 0.855 & 0.914 & 0.829 & 0.912 & 0.807 & 0.902 & 0.819 & 0.910 & 0.729 & 0.834 & 0.759 & 0.849 & \textbf{0.800} & \textbf{0.887} \\
            & LGD          & 0.801 & 0.874 & 0.819 & 0.905 & 0.799 & 0.897 & 0.802 & 0.899 & 0.730 & 0.835 & 0.757 & 0.848 & 0.785 & 0.876 \\
            & PL2          & 0.805 & 0.873 & 0.826 & 0.909 & 0.804 & 0.898 & 0.822 & 0.913 & 0.727 & 0.833 & 0.757 & 0.848 & 0.790 & 0.879 \\
            & TF\_IDF      & 0.807 & 0.876 & 0.823 & 0.907 & 0.801 & 0.896 & 0.814 & 0.907 & 0.726 & 0.832 & 0.755 & 0.847 & 0.788 & 0.877 \\
        \midrule
        \multirow{10}{*}{\rotatebox{90}{\texttt{precision@10}}}
            & BM25         & 0.765 & 0.828 & 0.807 & 0.881 & 0.792 & 0.872 & 0.807 & 0.885 & 0.657 & 0.756 & 0.697 & 0.785 & 0.754 & 0.835 \\
            & DFR\_BM25    & 0.768 & 0.833 & 0.806 & 0.880 & 0.791 & 0.870 & 0.807 & 0.884 & 0.658 & 0.757 & 0.696 & 0.785 & 0.754 & 0.835 \\
            & DFIZ         & 0.729 & 0.798 & 0.801 & 0.874 & 0.785 & 0.863 & 0.791 & 0.869 & 0.661 & 0.760 & 0.695 & 0.784 & 0.744 & 0.824 \\
            & DLH          & 0.802 & 0.862 & 0.817 & 0.889 & 0.803 & 0.881 & 0.823 & 0.899 & 0.666 & 0.764 & 0.697 & 0.785 & \textbf{0.768} & \textbf{0.847} \\
            & DPH          & 0.745 & 0.814 & 0.803 & 0.876 & 0.787 & 0.867 & 0.801 & 0.878 & 0.659 & 0.758 & 0.697 & 0.785 & 0.749 & 0.830 \\
            & DirichletLM  & 0.640 & 0.705 & 0.796 & 0.868 & 0.785 & 0.863 & 0.791 & 0.867 & 0.652 & 0.750 & 0.691 & 0.778 & 0.726 & 0.805 \\
            & Hiemstra\_LM & 0.819 & 0.880 & 0.813 & 0.886 & 0.799 & 0.878 & 0.815 & 0.892 & 0.658 & 0.757 & 0.699 & 0.786 & 0.767 & 0.846 \\
            & LGD          & 0.765 & 0.833 & 0.800 & 0.873 & 0.787 & 0.866 & 0.792 & 0.870 & 0.659 & 0.758 & 0.697 & 0.785 & 0.750 & 0.831 \\
            & PL2          & 0.766 & 0.829 & 0.811 & 0.885 & 0.796 & 0.876 & 0.819 & 0.896 & 0.656 & 0.755 & 0.698 & 0.786 & 0.758 & 0.838 \\
            & TF\_IDF      & 0.769 & 0.832 & 0.808 & 0.881 & 0.793 & 0.872 & 0.808 & 0.885 & 0.657 & 0.756 & 0.697 & 0.785 & 0.755 & 0.835 \\

        \bottomrule
    \end{tabular}}
    \renewcommand{\arraystretch}{1.0}
\end{table}

The next step in the transfer pipeline was to assign relevance scores to the passages of the selected documents from the previous step. This was done to identify the most relevant passages of a document with respect to the actual query associated with a document's relevance judgment. To achieve this, each passage was treated as an independent query and used to retrieve a document ranking from its original source corpus. Based on the retrieved document rankings, \texttt{precision@10} and \texttt{nDCG@10} were computed and assigned as passage scores. Since this study does not aim to optimize or analyze specific retriever systems but rather utilizes them for passage scoring, a diverse selection of models was tested, as listed in Table~\ref{tab:passage-scoring}.
\\\\
To evaluate the quality of the assigned passage scores and determine which retrieval model and metric combination are most effective in determining the relevance of passages, the rank correlation between the original relevance judgments and the newly assigned scores was computed using Kendall's $\tau$ and Spearman's $\rho$. Therefore, for each retriever-metric combination, an overall document score was aggregated from its passage scores by assigning a \makebox[\textwidth][l]{document the maximum passage score of its passages~\cite{craswell:2019}.}
\\\\
The rank correlation was then computed for all judged documents of each query individually, rather than across all relevance judgments of a retrieval task simultaneously, and then averaged. Macro-averaging across all queries is intended to reduce the effect of potential outliers for individual queries.
\\\\
The results in Table~\ref{tab:passage-scoring} show that all retrieval models achieved a very high rank correlation across all tested information retrieval tasks. While \texttt{Hiemstra\_LM} was the best-performing model, all models demonstrated strong performance based on the average $\tau$ and $\rho$ values. Therefore, the choice of retrieval model is not a critical factor for subsequent steps in the transfer pipeline, as all models produced passage scores with high rank correlation to the actual relevance labels from the retrieval tasks. This finding also applies to the \texttt{greedy} variant of both rank correlation methods, as shown in Table~\ref{tab:app:passage-scoring}. Although the \texttt{greedy} variant resulted in slightly higher rank correlation scores than the standard version, the overall behavior remained consistent.
\\\\
However, the choice of metric had a significant impact. \texttt{precision@10} consistently resulted in lower rank correlation compared to \texttt{nDCG@10}. This outcome was expected, as \texttt{precision@10} lacks the granularity needed to effectively differentiate between individual passages. Consequently, \texttt{nDCG@10} is the better metric for passage scoring. Therefore, in the following steps of the transfer pipeline, the passage scores assigned by \texttt{nDCG@10} alongside the retrieval model that achieved the highest rank correlation for each retrieval task is used.
\pagebreak


% ===================
% Candidate Selection
% ===================
\section{Candidate Selection}\label{eval-candidate-selection}

\begin{table}[t]
    \centering
    \footnotesize
    \caption{Overview of recall (Rec.) and the average number of candidate documents per query (Docs.) for all three candidate retrieval approaches, tested with document selection restricted to one passage per document (opd.) and without restriction (def.). The \texttt{Nearest Neighbor} and the \texttt{Union} approaches were also evaluated with varying $k$, determining the number of passages used for candidate retrieval.}
    \label{tab:candidate-selection}
    \resizebox{\textwidth}{!}{%
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{ccrcrcrcrcrcrcr}
        \toprule

        \multicolumn{3}{c}{\textbf{Approach}} & \multicolumn{2}{c}{\textbf{Touché 20}} & \multicolumn{2}{c}{\textbf{Robust04}} & \multicolumn{2}{c}{\textbf{TREC-7}} & \multicolumn{2}{c}{\textbf{TREC-8}} & \multicolumn{2}{c}{\textbf{TREC-19 DL}} & \multicolumn{2}{c}{\textbf{TREC-20 DL}} \\

        \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
                                          & & $k$ & Rec. & Docs. & Rec. & Docs. & Rec. & Docs. & Rec. & Docs. & Rec. & Docs. & Rec. & Docs. \\
        \midrule
        \multirow{2}{*}{\rotatebox{90}{\textbf{Naive}}} & \multirow{2}{*}{-} & \multirow{2}{*}{-} & \multirow{2}{*}{$0.912$} & \multirow{2}{*}{$1\,556$} & \multirow{2}{*}{$0.777$} & \multirow{2}{*}{$1\,572$} & \multirow{2}{*}{$0.657$} & \multirow{2}{*}{$1\,600$} & \multirow{2}{*}{$0.713$} & \multirow{2}{*}{$1\,566$} & \multirow{2}{*}{$0.736$} & \multirow{2}{*}{$976$} & \multirow{2}{*}{$0.751$} & \multirow{2}{*}{$955$} \\
        & & & & & & & & & & & & & & \\

        \midrule
        \multirow{6}{*}{\rotatebox{90}{\textbf{Nearest Neighbor}}} & \multirow{3}{*}{def.} &  10 & $0.609$ &  $97$ & $0.530$ &  $88$ & $0.430$ &  $85$ & $0.449$ &  $89$ & $0.560$ &  $81$ & $0.616$ &  $83$ \\
                                                                                        &  &  50 & $0.828$ & $318$ & $0.744$ & $334$ & $0.696$ & $349$ & $0.665$ & $356$ & $0.872$ & $268$ & $0.906$ & $269$ \\
                                                                                        &  & 100 & $0.978$ & $479$ & $0.833$ & $618$ & $0.787$ & $657$ & $0.757$ & $684$ & $0.966$ & $436$ & $0.974$ & $407$ \\
       \cmidrule(lr){2-15}
                                                                   & \multirow{3}{*}{opd.} &  10 & $0.699$ & $104$ & $0.584$ &  $96$ & $0.495$ &  $99$ & $0.489$ & $101$ & $0.560$ &  $81$ & $0.616$ &  $83$ \\
                                                                                        &  &  50 &   \boldmath$1.0$ & $209$ & $0.876$ & $360$ & $0.826$ & $419$ & $0.797$ & $439$ & $0.872$ & $268$ & $0.906$ & $269$ \\
                                                                                        &  & 100 &   \boldmath$1.0$ & $209$ & $0.885$ & $375$ & $0.826$ & $419$ & $0.797$ & $439$ & $0.966$ & $436$ & $0.974$ & $407$ \\

        \midrule
        \multirow{6}{*}{\rotatebox{90}{\textbf{Union}}} & \multirow{3}{*}{def.} &  10 & $0.931$ & $1\,577$ & $0.855$ & $1\,618$ & $0.771$ & $1\,651$ & $0.795$ & $1\,611$ & $0.859$ & $1\,018$ & $0.856$ &    $996$ \\
                                                                             &  &  50 & $0.948$ & $1\,678$ & $0.906$ & $1\,807$ & $0.861$ & $1\,860$ & $0.860$ & $1\,813$ & $0.954$ & $1\,146$ & $0.958$ & $1\,132$ \\
                                                                             &  & 100 & $0.992$ & $1\,774$ & $0.934$ & $2\,052$ & $0.904$ & $2\,134$ & $0.895$ & $2\,100$ & \boldmath$0.988$ & $1\,278$ & \boldmath$0.990$ & $1\,245$ \\
        \cmidrule(lr){2-15}
                                                        & \multirow{3}{*}{opd.} &  10 & $0.935$ & $1\,575$ & $0.865$ & $1\,622$ & $0.792$ & $1\,659$ & $0.808$ & $1\,621$ & $0.859$ & $1\,018$ & $0.856$ &    $996$ \\
                                                                             &  &  50 &   \boldmath$1.0$ & $1\,610$ & $0.950$ & $1\,827$ & \boldmath$0.915$ & $1\,916$ & \boldmath$0.919$ & $1\,888$ & $0.954$ & $1\,146$ & $0.958$ & $1\,132$ \\
                                                                             &  & 100 &   \boldmath$1.0$ & $1\,610$ & \boldmath$0.951$ & $1\,839$ & \boldmath$0.915$ & $1\,916$ & \boldmath$0.919$ & $1\,888$ & \boldmath$0.988$ & $1\,278$ & \boldmath$0.990$ & $1\,245$ \\

        \bottomrule 
    \end{tabular}}
    \renewcommand{\arraystretch}{1.0}
\end{table}

The next step in the transfer pipeline involved selecting documents from the target corpus in order to create new relevance judgments for each query. The goal is to identify documents highly likely to be relevant to a given query. This is crucial, as generating judgments for every query-document pair would be computationally expensive and unnecessary as unjudged documents in a test collection are mostly interpreted as not relevant by default.
\\\\
To maximize the number of likely relevant documents, three selection approaches were tested. The \texttt{Naive} approach retrieved the top 1000 documents from the target corpus using the query text and, when available, a query's narrative for an additional 1000 documents. The \texttt{Nearest Neighbor} approach used the top-scoring passages identified in Section~\ref{passage-scoring} as independent queries to retrieve the top 20 documents each from the target corpus as candidates. This approach was tested with different numbers of top passages per query: 10, 50, and 100. Additionally, it was tested with and without a restriction that limited each query's top passages to a maximum of one passage per document (\texttt{opd.}). The \texttt{Union} approach combined all documents retrieved by the \texttt{Naive} and each \texttt{Nearest Neighbor} approach, filtering out duplicate candidates.
\\\\
A recall evaluation of the candidate selection approaches is only possible for the relevance transfer within the same dataset, meaning from a source dataset to its own document corpus. This constraint is because existing relevance judgments must be available for the same retrieval task, i.e., for the same queries, in both the source and target datasets. Consequently, Table~\ref{tab:candidate-selection} presents recall and the average number of selected candidate documents per query of the transfer pipeline within the same dataset. The recall metric is computed as the proportion of retrieved candidate documents that are judged as relevant relative to the total number of relevant documents in the retrieval task.
\\\\
The \texttt{Naive} approach achieved very high recall for \texttt{Touché 20}, benefiting from its relatively small document corpus of 0.4~million documents and the retrieval of up to 2000 documents per query. However, its performance reduces significantly on all other datasets, because of much larger corpora. Additionally, \texttt{TREC-19 DL} and \texttt{TREC-20 DL} do not provide query narratives, limiting retrieval to a maximum of 1000 documents per query and thereby reducing recall. Overall, the \texttt{Naive} approach was able to retrieve a large amount of relevant documents, but this comes at the cost of including many non-relevant candidates, making the selection less efficient.
\\\\
The \texttt{Nearest Neighbor} approach outperformed \texttt{Naive} in nearly all variants, except when using only the top 10 passages, limiting candidate documents per query to 200. For \texttt{TREC-7} and \texttt{TREC-8}, the number of selected documents did not increase between the \texttt{opd.\ }top 50 and top 100 variants due to the document selection process evaluated in Section~\ref{eval-document-selection}. As shown in Table~\ref{tab:document-selection}, these retrieval tasks have only one positive relevance label and are restricted to a maximum of 50 relevant documents per query. Thus, using the top 100 passages did not improve recall, as no additional relevant passages available under the \texttt{opd.\ }constraint. For \texttt{Touché 20}, which has two positive relevance labels, a similar effect was observed due to its overall low number of relevance judgments. However, even if \texttt{Robust04} averages 36 relevant documents, some queries exceed 50 relevant judgments in total, which explains the higher recall achieved with the \texttt{opd.\ 100} variant. Another expected outcome of Table~\ref{tab:candidate-selection} is that the same recall is achieved for \texttt{TREC-19 DL} and \texttt{TREC-20 DL} for the \texttt{def.} and \texttt{opd.\ }variants. This is because their document corpora consist solely of passages, making the restriction of one passage per document redundant. Overall, a key finding is that restricting the top passages to a maximum of one passage per document (\texttt{opd.}) consistently improved performance compared to allowing multiple passages per document (\texttt{def.}). This result shows enforcing greater document diversity leads to more effective candidate selection. 
\\\\
The \texttt{Union} approach further improved recall, particularly for \texttt{Robust04}, \texttt{TREC-7}, and \texttt{TREC-8}, where it increased recall by approximately 10 percentage points. For \texttt{TREC-19 DL} and \texttt{TREC-20 DL}, the \texttt{Union opd.\ 100} variant improved recall by three percentage points compared to \texttt{Union opd.\ 50}, reaching an impressive recall of approximately $99\%$. However, this improvement came at the cost of tripling or quadrupling the number of retrieved documents per query. Despite this increase, the trade-off remains acceptable, as recall is consistently high across all six tasks, and the number of retrieved documents per query remains below 2000. Therefore, \texttt{Union opd.\ 100} is the most effective candidate selection approach and is used in the subsequent step of the pipeline.

% ====================
% Pairwise Preferences
% ====================
\section{Pairwise Preferences}\label{eval-pairwise-preferences}

The candidate selection process identified \mbox{\texttt{Union opd.\ 100}} as the most effective approach for pre-selecting likely relevant documents from the target corpus. Consequently, the evaluation in this section is conducted using the candidate documents selected by this approach. For further processing with large language models, all candidate documents were segmented into passages, following the same \mbox{procedure as described in Section~\ref{segmentation-with-spacy}.}
\\\\
For each query in a retrieval task, all passages from the selected candidate documents were paired with the 15 most relevant and 5 least relevant passages for that query. These 20 passages were determined through the passage-scoring step described in Section~\ref{passage-scoring}. As evaluated in Section~\ref{eval-passage-scoring}, the passage scores were derived from the retriever-metric combination with the highest rank correlation, ensuring the selection of the 15 highest-scoring relevant and the 5 lowest-scoring non-relevant passages per query. Since Section~\ref{eval-candidate-selection} showed that nearest neighbor approaches performed better when limited to selecting at most one passage per document, this restriction was also applied when selecting the 20 passages, as outlined in Section~\ref{composing-final-candidates} under \texttt{Diversified Selection}.
\\\\
To determine the relevance of candidate documents from the target corpus, tuples consisting of \texttt{(query, known passage, passage to judge)}, were processed using various versions of Google's \texttt{T5} model. For each tuple, the model was prompted to predict the relevance of the target passage to the query, based on the known source passage. After processing all comparisons, each  \linebreak target passage received 20 relevance scores, one for each comparison \linebreak to a selected source passage. To derive an overall passage score, these individual scores were aggregated and transformed using different methods. \linebreak  The tested aggregation methods included \texttt{mean, min, max, sum}, while the transformation methods included \texttt{id, log, exp, sqrt}. Finally, a document's relevance score was determined by taking the maximum of all its passage scores, based on the assumption that a document is only as relevant as its \mbox{most relevant passage~\cite{craswell:2019}}.
\\\\
For comparison, a pointwise approach was tested alongside the pairwise preference approach. In this setup, the \texttt{T5} models received only the query and the target passage, without comparisons to known passages, and was prompted to determine the target passage's relevance to the query. Since this approach evaluated each passage independently, no aggregation or transformation was needed at passage level. However, to derive the final document relevance score, the maximum of all passage scores for the document was used again.

% Transfer Pipeline to Source Corpus itself
\subsection{Transfer Pipeline on Source Corpora}\label{eval-pairwise-preferences-source}

The evaluated rank correlation scores between the actual relevance labels of the source datasets and the generated relevance scores from the self-transfer are listed in Table~\ref{tab:self-pairwise-preferences}. These scores represent the average rank correlation across all queries for each retrieval task. The rank correlation for each query was computed based on all candidates from the \texttt{Union opd.\ 100} approach that had a relevance judgment in the corresponding retrieval task.
\\\\
Overall, the best aggregation method across all three transformer models was \texttt{min}, which achieved the highest rank correlation across all retrieval tasks, except one for \texttt{t5-small} with \texttt{max} aggregation. This result suggests that a passage is only as relevant as its least relevant comparison to the source passages. Additionally, the table shows that applying transformation methods to the aggregated passage scores had no impact on the rank correlation evaluation. This is expected, as tested transformations keep the linear orders of the relevance scores, meaning that applying the same transformation to all aggregated scores has no effect on rank correlation, making the step redundant.
\\\\
One notable outlier is \texttt{Touché 20}, which has two key differences from the other evaluated datasets. First, rank correlation scores using \texttt{t5-small} were higher than those for \texttt{flan-t5-small} and \texttt{flan-t5-base}. Second, the pointwise approach for \texttt{flan-t5-base} achieved the overall best rank correlation for \mbox{\texttt{Touché 20}}. Unlike the other datasets, \texttt{Touché 20} has the smallest document corpus and the fewest relevance judgments per query among all tested retrieval tasks. Additionally, this dataset is known to be noisy and has shown unusual behavior in other analyses as well~\cite{thakur:2024}.
\begin{table}[t!]
    \centering
    \footnotesize
    \caption{Rank correlations of the inferred relevance judgements to the original document judgments reported in terms of Kendall's $\tau$ and Spearman's $\rho$.}
    \label{tab:self-pairwise-preferences}
    \resizebox{\textwidth}{!}{%
    \renewcommand{\arraystretch}{1.0}
    \begin{tabular}{cclccccccccccccc}
        \toprule

        \multicolumn{3}{c}{\textbf{Approach}} & \multicolumn{2}{c}{\textbf{Touché 20}} & \multicolumn{2}{c}{\textbf{Robust04}} & \multicolumn{2}{c}{\textbf{TREC-7}} & \multicolumn{2}{c}{\textbf{TREC-8}} & \multicolumn{2}{c}{\textbf{TREC-19 DL}} & \multicolumn{2}{c}{\textbf{TREC-20 DL}} \\
        \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
                            & & & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ \\
        \midrule
        \multirow{16}{*}{\rotatebox{90}{\makecell{\textbf{Pairwise Preferences} \\ \textbf{flan-t5-base}}}}
        & \multirow{4}{*}{\textbf{mean}}
                  & \textbf{id}   & 0.232 & 0.288 & 0.154 & 0.189 & 0.126 & 0.154 & 0.174 & 0.213 & 0.254 & 0.316 & 0.233 & 0.293 \\
                & & \textbf{log}  & 0.232 & 0.288 & 0.154 & 0.189 & 0.126 & 0.154 & 0.174 & 0.213 & 0.254 & 0.316 & 0.233 & 0.293 \\
                & & \textbf{exp}  & 0.232 & 0.288 & 0.154 & 0.189 & 0.126 & 0.154 & 0.174 & 0.213 & 0.254 & 0.316 & 0.233 & 0.293 \\
                & & \textbf{sqrt} & 0.232 & 0.288 & 0.154 & 0.189 & 0.126 & 0.154 & 0.174 & 0.213 & 0.254 & 0.316 & 0.233 & 0.293 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{min}}
                  & \textbf{id}   & \textbf{0.250} & \textbf{0.314} & \textbf{0.169} & \textbf{0.208} & \textbf{0.137} & \textbf{0.167} & \textbf{0.187} & \textbf{0.229} & \textbf{0.311} & \textbf{0.387} & \textbf{0.293} & \textbf{0.366} \\
                & & \textbf{log}  & \textbf{0.250} & \textbf{0.314} & \textbf{0.169} & \textbf{0.208} & \textbf{0.137} & \textbf{0.167} & \textbf{0.187} & \textbf{0.229} & \textbf{0.311} & \textbf{0.387} & \textbf{0.293} & \textbf{0.366} \\
                & & \textbf{exp}  & \textbf{0.250} & \textbf{0.314} & \textbf{0.169} & \textbf{0.208} & \textbf{0.137} & \textbf{0.167} & \textbf{0.187} & \textbf{0.229} & \textbf{0.311} & \textbf{0.387} & \textbf{0.293} & \textbf{0.366} \\
                & & \textbf{sqrt} & \textbf{0.250} & \textbf{0.314} & \textbf{0.169} & \textbf{0.208} & \textbf{0.137} & \textbf{0.167} & \textbf{0.187} & \textbf{0.229} & \textbf{0.311} & \textbf{0.387} & \textbf{0.293} & \textbf{0.366} \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{max}}
                  & \textbf{id}   & 0.226 & 0.283 & 0.079 & 0.097 & 0.084 & 0.103 & 0.080 & 0.098 & 0.107 & 0.136 & 0.120 & 0.151 \\
                & & \textbf{log}  & 0.226 & 0.283 & 0.079 & 0.097 & 0.084 & 0.103 & 0.080 & 0.098 & 0.107 & 0.136 & 0.120 & 0.151 \\
                & & \textbf{exp}  & 0.226 & 0.283 & 0.079 & 0.097 & 0.084 & 0.103 & 0.080 & 0.098 & 0.107 & 0.136 & 0.120 & 0.151 \\
                & & \textbf{sqrt} & 0.226 & 0.283 & 0.079 & 0.097 & 0.084 & 0.103 & 0.080 & 0.098 & 0.107 & 0.136 & 0.120 & 0.151 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{sum}}
                  & \textbf{id}   & 0.232 & 0.288 & 0.154 & 0.189 & 0.126 & 0.154 & 0.174 & 0.213 & 0.254 & 0.316 & 0.233 & 0.293 \\
                & & \textbf{log}  & 0.232 & 0.288 & 0.154 & 0.189 & 0.126 & 0.154 & 0.174 & 0.213 & 0.254 & 0.316 & 0.233 & 0.293 \\
                & & \textbf{exp}  & 0.232 & 0.288 & 0.154 & 0.189 & 0.126 & 0.154 & 0.174 & 0.213 & 0.254 & 0.316 & 0.233 & 0.293 \\
                & & \textbf{sqrt} & 0.232 & 0.288 & 0.154 & 0.189 & 0.126 & 0.154 & 0.174 & 0.213 & 0.254 & 0.316 & 0.233 & 0.293 \\
        \midrule

        \multirow{16}{*}{\rotatebox{90}{\makecell{\textbf{Pairwise Preferences} \\ \textbf{flan-t5-small}}}}
        & \multirow{4}{*}{\textbf{mean}}
                  & \textbf{id}   & 0.232 & 0.299 & 0.067 & 0.082 & 0.065 & 0.079 & 0.082 & 0.101 & 0.175 & 0.220 & 0.132 & 0.165 \\
                & & \textbf{log}  & 0.232 & 0.299 & 0.067 & 0.082 & 0.065 & 0.079 & 0.082 & 0.101 & 0.175 & 0.220 & 0.132 & 0.165 \\
                & & \textbf{exp}  & 0.232 & 0.299 & 0.067 & 0.082 & 0.065 & 0.079 & 0.082 & 0.101 & 0.175 & 0.220 & 0.132 & 0.165 \\
                & & \textbf{sqrt} & 0.232 & 0.299 & 0.067 & 0.082 & 0.065 & 0.079 & 0.082 & 0.101 & 0.175 & 0.220 & 0.132 & 0.165 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{min}}
                  & \textbf{id}   & \textbf{0.301} & \textbf{0.388} & \textbf{0.082} & \textbf{0.101} & \textbf{0.080} & \textbf{0.097} & \textbf{0.094} & \textbf{0.115} & \textbf{0.224} & \textbf{0.282} & \textbf{0.179} & \textbf{0.224} \\
                & & \textbf{log}  & \textbf{0.301} & \textbf{0.388} & \textbf{0.082} & \textbf{0.101} & \textbf{0.080} & \textbf{0.097} & \textbf{0.094} & \textbf{0.115} & \textbf{0.224} & \textbf{0.282} & \textbf{0.179} & \textbf{0.224} \\
                & & \textbf{exp}  & \textbf{0.301} & \textbf{0.388} & \textbf{0.082} & \textbf{0.101} & \textbf{0.080} & \textbf{0.097} & \textbf{0.094} & \textbf{0.115} & \textbf{0.224} & \textbf{0.282} & \textbf{0.179} & \textbf{0.224} \\
                & & \textbf{sqrt} & \textbf{0.301} & \textbf{0.388} & \textbf{0.082} & \textbf{0.101} & \textbf{0.080} & \textbf{0.097} & \textbf{0.094} & \textbf{0.115} & \textbf{0.224} & \textbf{0.282} & \textbf{0.179} & \textbf{0.224} \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{max}}
                  & \textbf{id}   & 0.196 & 0.250 & 0.028 & 0.035 & 0.030 & 0.037 & 0.043 & 0.052 & 0.073 & 0.092 & 0.048 & 0.060 \\
                & & \textbf{log}  & 0.196 & 0.250 & 0.028 & 0.035 & 0.030 & 0.037 & 0.043 & 0.052 & 0.073 & 0.092 & 0.048 & 0.060 \\
                & & \textbf{exp}  & 0.196 & 0.250 & 0.028 & 0.035 & 0.030 & 0.037 & 0.043 & 0.052 & 0.073 & 0.092 & 0.048 & 0.060 \\
                & & \textbf{sqrt} & 0.196 & 0.250 & 0.028 & 0.035 & 0.030 & 0.037 & 0.043 & 0.052 & 0.073 & 0.092 & 0.048 & 0.060 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{sum}}
                  & \textbf{id}   & 0.232 & 0.299 & 0.067 & 0.082 & 0.065 & 0.079 & 0.082 & 0.101 & 0.175 & 0.220 & 0.132 & 0.165 \\
                & & \textbf{log}  & 0.232 & 0.299 & 0.067 & 0.082 & 0.065 & 0.079 & 0.082 & 0.101 & 0.175 & 0.220 & 0.132 & 0.165 \\
                & & \textbf{exp}  & 0.232 & 0.299 & 0.067 & 0.082 & 0.065 & 0.079 & 0.082 & 0.101 & 0.175 & 0.220 & 0.132 & 0.165 \\
                & & \textbf{sqrt} & 0.232 & 0.299 & 0.067 & 0.082 & 0.065 & 0.079 & 0.082 & 0.101 & 0.175 & 0.220 & 0.132 & 0.165 \\
        \midrule

        \multirow{16}{*}{\rotatebox{90}{\makecell{\textbf{Pairwise Preferences} \\ \textbf{t5-small}}}}
        & \multirow{4}{*}{\textbf{mean}}
                  & \textbf{id}   & 0.239 & 0.308 & -0.003 & -0.003 & 0.001 & 0.001 & -0.005 & -0.006 & -0.050 & -0.063 & -0.042 & -0.053 \\
                & & \textbf{log}  & 0.239 & 0.308 & -0.003 & -0.003 & 0.001 & 0.001 & -0.005 & -0.006 & -0.050 & -0.063 & -0.042 & -0.053 \\
                & & \textbf{exp}  & 0.239 & 0.308 & -0.003 & -0.003 & 0.001 & 0.001 & -0.005 & -0.006 & -0.050 & -0.063 & -0.042 & -0.053 \\
                & & \textbf{sqrt} & 0.239 & 0.308 & -0.003 & -0.003 & 0.001 & 0.001 & -0.005 & -0.006 & -0.050 & -0.063 & -0.042 & -0.053 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{min}}
                  & \textbf{id}   & \textbf{0.320} & \textbf{0.410} & \textbf{0.010} & \textbf{0.013} & \textbf{0.025} & \textbf{0.031} & \textbf{0.008} & \textbf{0.010} & -0.039 & -0.050 & \textbf{-0.019} & \textbf{-0.024} \\
                & & \textbf{log}  & \textbf{0.320} & \textbf{0.410} & \textbf{0.010} & \textbf{0.013} & \textbf{0.025} & \textbf{0.031} & \textbf{0.008} & \textbf{0.010} & -0.039 & -0.050 & \textbf{-0.019} & \textbf{-0.024} \\
                & & \textbf{exp}  & \textbf{0.320} & \textbf{0.410} & \textbf{0.010} & \textbf{0.013} & \textbf{0.025} & \textbf{0.031} & \textbf{0.008} & \textbf{0.010} & -0.039 & -0.050 & \textbf{-0.019} & \textbf{-0.024} \\
                & & \textbf{sqrt} & \textbf{0.320} & \textbf{0.410} & \textbf{0.010} & \textbf{0.013} & \textbf{0.025} & \textbf{0.031} & \textbf{0.008} & \textbf{0.010} & -0.039 & -0.050 & \textbf{-0.019} & \textbf{-0.024} \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{max}}
                  & \textbf{id}   & 0.115 & 0.150 & -0.002 & -0.002 & -0.002 & -0.002 & -0.004 & -0.005 & \textbf{-0.029} & \textbf{-0.037} & -0.042 & -0.053 \\
                & & \textbf{log}  & 0.115 & 0.150 & -0.002 & -0.002 & -0.002 & -0.002 & -0.004 & -0.005 & \textbf{-0.029} & \textbf{-0.037} & -0.042 & -0.053 \\
                & & \textbf{exp}  & 0.115 & 0.150 & -0.002 & -0.002 & -0.002 & -0.002 & -0.004 & -0.005 & \textbf{-0.029} & \textbf{-0.037} & -0.042 & -0.053 \\
                & & \textbf{sqrt} & 0.115 & 0.150 & -0.002 & -0.002 & -0.002 & -0.002 & -0.004 & -0.005 & \textbf{-0.029} & \textbf{-0.037} & -0.042 & -0.053 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{sum}}
                  & \textbf{id}   & 0.239 & 0.308 & -0.003 & -0.003 & 0.001 & 0.001 & -0.005 & -0.006 & -0.050 & -0.063 & -0.042 & -0.053 \\
                & & \textbf{log}  & 0.239 & 0.308 & -0.003 & -0.003 & 0.001 & 0.001 & -0.005 & -0.006 & -0.050 & -0.063 & -0.042 & -0.053 \\
                & & \textbf{exp}  & 0.239 & 0.308 & -0.003 & -0.003 & 0.001 & 0.001 & -0.005 & -0.006 & -0.050 & -0.063 & -0.042 & -0.053 \\
                & & \textbf{sqrt} & 0.239 & 0.308 & -0.003 & -0.003 & 0.001 & 0.001 & -0.005 & -0.006 & -0.050 & -0.063 & -0.042 & -0.053 \\
        \midrule

        \multirow{5}{*}{\rotatebox{90}{\makecell{\textbf{Pointwise} \\ \textbf{Preferences}}}}
            & & & & & & & & & & & & & & \\
            & \multicolumn{2}{c}{\textbf{flan-t5-base}}  & 0.350 & 0.440 & -0.013 & -0.016 & -0.003 & -0.004 & -0.026 & -0.032 & 0.037 & 0.048 & 0.031 & 0.039 \\
            & \multicolumn{2}{c}{\textbf{flan-t5-small}} & -0.127 & -0.162 & -0.011 & -0.014 & -0.001 & -0.001 & -0.018 & -0.022 & -0.048 & -0.061 & -0.031 & -0.040 \\
            & \multicolumn{2}{c}{\textbf{t5-small}}      & -0.152 & -0.194 & 0.004 & 0.005 & 0.013 & 0.015 & 0.005 & 0.006 & -0.056 & -0.070 & -0.042 & -0.053 \\
            & & & & & & & & & & & & & & \\
        \bottomrule 
    \end{tabular}}
    \renewcommand{\arraystretch}{1.0}
\end{table}
\\\\\\
The retrieval tasks based on \texttt{Disks4+5} and \texttt{MS MARCO} produced highly acceptable results. In all cases, the most advanced transformer model, \texttt{flan-t5-base}, outperformed all other models using the pairwise preference approach. A key finding from this evaluation is that the pointwise preference approach performed poorly across all tested models, showing almost no rank correlation between predicted relevance and actual labels. This highlights the limitation of pointwise approaches in predicting document relevance based solely on the document itself. In contrast, the pairwise preference approach achieved notable rank correlation values. Specifically, for \texttt{flan-t5-base}, the retrieval tasks based on the \texttt{Disks4+5} corpus reached rank correlations of up to $0.229$. For the retrieval tasks based on the \texttt{MS MARCO} passage corpus, the inferred relevance scores achieved rank correlations of approximately $0.3$ for Kendall's $\tau$ and $3.7$ for Spearman's $\rho$, indicating a weak to moderate rank correlation.
\\\\
The high correlation scores for \texttt{TREC-DL} 19 and \texttt{TREC-DL 20}, when compared to the other datasets, except \texttt{Touché 20}, are likely due to two factors. First, as analyzed in Section~\ref{eval-document-selection}, these retrieval tasks have a relatively high number of judgments per query, providing a strong foundation for relevance transfer. Second, the \texttt{MS MARCO} corpus is a passage corpus, meaning all documents processed by the transfer pipeline were already given as polished passages. As a result, the pairwise comparisons performed the evaluation on individual passages for \texttt{MS MARCO} rather than segmented sections of longer documents. Additionally, the segmentation using \texttt{spaCy}  segments text based on punctuation and maximum passage length which is more rudimentary. Since this work does not include an evaluation of document segmentation methods, the question of optimal document segmentation remains open for future research.
\\\\
The evaluation was also conducted using the \texttt{greedy} versions of Kendall's and Spearman's rank correlations, as shown in Table~\ref{tab:app:self-pairwise-preferences}. Across all datasets, these scores were slightly higher, but the trends observed with the default Kendall and Spearman metrics remained consistent.
\pagebreak
\begin{figure}[t!]
    \centering
    \begin{tabular}{ccc}
        \textbf{Pairwise Preferences} & \hspace{2cm} & \textbf{Pointwise Preferences} \\
    \end{tabular}
    \footnotesize
    % First row
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pairwise_self_score_distribution_flan-t5-base.pdf}
        \label{fig:pairwise_flan-t5-base}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pointwise_self_score_distribution_flan-t5-base.pdf}
        \label{fig:pointwise_flan-t5-base}
    \end{subfigure}

    \vspace{-0.5cm}
    \textbf{(a)} Relevance scores generated using \texttt{flan-t5-base}.
    \vspace{0.5cm}

    % Second row
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pairwise_self_score_distribution_flan-t5-small.pdf}
        \label{fig:pairwise_flan-t5-small}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pointwise_self_score_distribution_flan-t5-small.pdf}
        \label{fig:pointwise_flan-t5-small}
    \end{subfigure}

    \vspace{-0.5cm}
    \textbf{(b)} Relevance scores generated using \texttt{flan-t5-small}.
    \vspace{0.5cm}

    % Third row
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pairwise_self_score_distribution_t5-small.pdf}
        \label{fig:pairwise_t5-small}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pointwise_self_score_distribution_t5-small.pdf}
        \label{fig:pointwise_t5-small}
    \end{subfigure}

    \vspace{-0.5cm}
    \textbf{(c)} Relevance scores generated using \texttt{t5-small}.
    \vspace{0.5cm}

    \caption{Distributions of the inferred relevance scores for self-transfer across all six retrieval tasks, using the pairwise and pointwise approaches with different versions of the \texttt{T5} model.}
    \label{fig:score-distributions}
\end{figure}
\\\\\\
To better analyze the rank correlation results, Figure~\ref{fig:score-distributions} visualizes the distribution of inferred relevance scores for both the pairwise and pointwise approaches across all six retrieval tasks. In general, the pairwise approaches have greater score variance than the pointwise approaches. This is likely because the pointwise models generate only one relevance score per passage, whereas the pairwise models compare each passage against 20 source passages, leading to more granular relevance differentiation. The comparison to 20 source passages rather than solely on the target passage itself also explains the overall higher number of scores in the pairwise approaches.
\\\\
When comparing pointwise models, it is evident that more advanced models introduce greater variance into the score distribution. However, the pairwise approaches exhibit significantly higher score variance overall. Looking at the distribution of the pairwise models, notable differences emerge. While \texttt{t5-small} tends to underestimate relevance scores, \texttt{flan-t5-small} assigns generally higher scores on average. In contrast, \texttt{flan-t5-base} produces the highest score variance, allowing for more effective passage differentiation. Additionally, unlike \texttt{flan-t5-small} or \texttt{t5-small}, its score distribution is more spread out rather than forming a sharp peak, suggesting a more nuanced relevance assessment. This broader score distribution is a key reason why \texttt{flan-t5-base} achieved the best overall results for both standard and \texttt{greedy} rank correlation. In conclusion, the ability to infer a broad spectrum of relevance scores seems to allow for better passage differentiation, reinforcing the idea that a passage's overall relevance is dependent on its weakest passage regarding that the \texttt{min} aggregation performed best.

\pagebreak
% Transfer Pipeline to ClueWeb22/b
\subsection{Transfer Pipeline on \texttt{ClueWeb22/b}}\label{eval-pairwise-preferences-target}

\begin{table}[t]
    \centering
    \footnotesize
    \caption{Empirically selected queries from the six retrieval tasks used in the transfer pipeline to generate new relevance judgments for \texttt{ClueWeb22/b}.}
    \label{tab:cw22-queries}
    \begin{tabular}{crl}
        \toprule
        \textbf{Dataset} & \textbf{Query ID} & \textbf{Query Text} \\
        \midrule
        
        \multirow{2}{*}{\textbf{Touché 20}}
            &      34 & \glqq Are social networking sites good for our society?\grqq{} \\
            &      49 & \glqq Should body cameras be mandatory for police?\grqq{} \\
        \midrule
        \multirow{2}{*}{\textbf{Robust04}}
            &     448 & \glqq ship losses\grqq{} \\
            &     681 & \glqq wind power location\grqq{} \\
        \midrule
        \multirow{2}{*}{\textbf{TREC-7}}
            &     354 & \glqq journalist risks\grqq{} \\
            &     358 & \glqq blood-alcohol fatalities\grqq{} \\
        \midrule
        \multirow{2}{*}{\textbf{TREC-8}}
            &     422 & \glqq art, stolen, forged\grqq{} \\
            &     441 & \glqq Lyme disease\grqq{} \\
        \midrule
        \multirow{2}{*}{\textbf{TREC-19 DL}}
            & 1037798 & \glqq who is robert gray\grqq{} \\
            & 1129237 & \glqq hydrogen is a liquid below what temperature\grqq{} \\
        \midrule
        \multirow{2}{*}{\textbf{TREC-20 DL}}
            &  997622 & \glqq where is the show shameless filmed\grqq{} \\
            & 1051399 & \glqq who sings monk theme song\grqq{} \\
            & 1127540 & \glqq meaning of shebang\grqq{} \\

        \bottomrule
    \end{tabular}
\end{table}

The transfer to \texttt{ClueWeb22/b} followed the same setup as the self-transfer evaluation. However, a key challenge in this transfer was the absence of existing relevance judgments for the queries of the six tested retrieval tasks needed for evaluation. To address this issue, two to three queries from each retrieval task were manually selected, as shown in Table~\ref{tab:cw22-queries}. The relevance transfer pipeline was then applied to generate new relevance judgments for documents from the \texttt{ClueWeb22/b} corpus for these queries.
\\\\
Since the \texttt{Union opd.\ 100} candidate selection was identified as the most effective approach in Section~\ref{eval-candidate-selection}, it was again used to determine candidate documents within the \texttt{ClueWeb22/b} corpus. After selecting the candidate documents for the chosen queries, a pooling process was conducted. Twelve lexical models\footnote{\scriptsize BM25, DFIC, DFIZ, DirichletLM, DFRee, DLH, DPH, Hiemstra\_LM, InB2, LGD, Js\_KLs, PL2} and three neural models\footnote{\scriptsize ANCE Base Cosine, MonoT5 Base, MonoBERT Base} were utilized. The 15 pooled retrieval models were executed in their archived version from \texttt{TIRA/TIREx}~\citep{froebe:2023b,froebe:2023c}. From that pooling, a top-10 pool of the 15 systems was created using trectools~\citep{palotti:2019}. The remaining documents after pooling were then manually assessed to determine their relevance to the corresponding queries.
\\\\\\
For this assessment, the top three passages of each pooled document, determined by the inferred relevance scores from the pairwise preference approach using \texttt{flan-t5-base}, were presented to a human judge. The judge then assigned a relevance label of 0, 1, or 2 to each passage, where 0 indicated non-relevance, 1 indicated relevance, and 2 indicated high relevance. The highest relevance label among the three passages was then assigned as the document's relevance judgment. These relevance judgments were used to evaluate the rank correlation between the inferred relevance scores and the manually assessed \mbox{relevance} labels for \texttt{ClueWeb22/b}.
\\\\
The rank correlation scores between the inferred relevance judgments and the manually assessed relevance labels are listed in Table~\ref{tab:cw22-pairwise-preferences}. The transfer process to \texttt{ClueWeb22/b} exhibited similar behavior to the self-transfer results in \mbox{Table~\ref{tab:self-pairwise-preferences}}. Additionally, the distribution of inferred relevance scores for the pooled candidates, shown in Figure~\ref{fig:score-distributions-cw22}, closely matched the self-transfer distribution in Figure~\ref{fig:score-distributions}. Again, the pairwise preference approach using \texttt{flan-t5-base} achieved the highest rank correlation scores, yielding moderate to good correlations. Despite some outliers, the \texttt{min} aggregation method once again proved to be the most effective. Unlike previous observations, \mbox{\texttt{Touché 20}} aligned with the other retrieval tasks in this evaluation, showing no unusual behavior. However, \texttt{Robust04} stood out as an outlier, as the pointwise approach with \texttt{t5-small} unexpectedly achieved the highest rank correlation, while pointwise approaches overall exhibited little to no rank correlation.
\\\\
During the annotation process for \texttt{Robust04}, it was observed that relevant documents did not transfer well from its news domain to the general-purpose web domain of \texttt{ClueWeb22/b}, as only a few documents were judged as relevant. This is likely due to the brevity of query 448 and 681, which consist of only two or three words, creating a significant gap between the query text and its narrative. Since the pairwise preference approach relied solely on the query text, while the annotator had access to the full description and narrative, this lack of contextual information likely impacted the model's ability to infer relevance accurately. A potential improvement could be to incorporate the description and/or narrative into the \texttt{DuoPrompt} to provide the model with a richer understanding of the query, allowing for more precise relevance \mbox{judgments}.
% During the annotation process for \texttt{Robust04}, it was observed that the relevant documents did not transfer well from the news domain of \texttt{Robust04} to the general-purpose web domain of \texttt{ClueWeb22/b} because only few documents from \texttt{ClueWeb22/b} were judged as relevant. The reason for this is likely due to the nature of the queries 448 and 681, each consisting of only two or three words. While the pairwise preference approach relied solely on the query text, the annotator had access to additional context, including the query description and narrative. It was determined found that the gap between the query text and the narrative is big, for instance, for query 448, is only \textit{\glqq ship losses \grqq{}}, but the narrative describes the query as \textit{\glqq Any ship loss due to weather is relevant, either in internationa\textbackslash nor coastal waters \grqq{}}. The same applies to query 681, where the narratives is much more releling on which documents are relevant. A potential improvement could involve incorporating the description and/or narrative into the \texttt{DuoPrompt}, providing the model with more information to make more informed relevance assessments. However, this remains an avenue for future research.
% During the annotation process for \texttt{Robust04}, it was observed that  the documents for its selected queries were particularly difficult to judge. This challenge was due to the nature of the queries, which were either extremely broad, such as query 448, or highly specific, such as query 684, each consisting of only two or three words. While the pairwise preference approach relied solely on the query text, the annotator had access to additional context, including the query description and narrative. A potential improvement could involve incorporating the description and/or narrative into the \texttt{DuoPrompt}, providing the model with more information to make more informed relevance assessments. However, this remains an avenue for future research.
\\\\
Table~\ref{tab:app:cw22-pairwise-preferences} presents the rank correlation scores for the \texttt{greedy} versions of Kendall's $\tau$ and Spearman's $\rho$. The greedy mapping of the inferred relevance scores to the relevance labels 0, 1, or 2 achieved strong agreement in both pairwise and pointwise, with the manually created judgments. The pairwise approach still outperforms the pointwise approach.

\begin{table}[t!]
    \centering
    \footnotesize
    \caption{Rank correlations between inferred relevance judgments and manually created judgments for \texttt{ClueWeb22/b}, reported using Kendall's $\tau$ and Spearman's $\rho$.}
    \label{tab:cw22-pairwise-preferences}
    \resizebox{\textwidth}{!}{%
    \renewcommand{\arraystretch}{1.0}
    \begin{tabular}{cclccccccccccccc}
        \toprule

        \multicolumn{3}{c}{\textbf{Approach}} & \multicolumn{2}{c}{\textbf{Touché 20}} & \multicolumn{2}{c}{\textbf{Robust04}} & \multicolumn{2}{c}{\textbf{TREC-7}} & \multicolumn{2}{c}{\textbf{TREC-8}} & \multicolumn{2}{c}{\textbf{TREC-19 DL}} & \multicolumn{2}{c}{\textbf{TREC-20 DL}} \\
        \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
                            & & & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ & $\tau$ & $\rho$ \\
        \midrule
        \multirow{16}{*}{\rotatebox{90}{\makecell{\textbf{Pairwise Preferences} \\ \textbf{flan-t5-base}}}}
        & \multirow{4}{*}{\textbf{mean}}
                  & \textbf{id}   & \textbf{0.330} & 0.411 & -0.098 & -0.114 & 0.273 & 0.342 & 0.132 & 0.166 & 0.305 & 0.389 & 0.052 & 0.059 \\
                & & \textbf{log}  & \textbf{0.330} & 0.411 & -0.098 & -0.114 & 0.273 & 0.342 & 0.132 & 0.166 & 0.305 & 0.389 & 0.052 & 0.059 \\
                & & \textbf{exp}  & \textbf{0.330} & 0.411 & -0.098 & -0.114 & 0.273 & 0.342 & 0.132 & 0.166 & 0.305 & 0.389 & 0.052 & 0.059 \\
                & & \textbf{sqrt} & \textbf{0.330} & 0.411 & -0.098 & -0.114 & 0.273 & 0.342 & 0.132 & 0.166 & 0.305 & 0.389 & 0.052 & 0.059 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{min}}
                  & \textbf{id}   & 0.329 & \textbf{0.417} & \textbf{-0.080} & \textbf{-0.096} & \textbf{0.298} & \textbf{0.363} & \textbf{0.263} & \textbf{0.322} & \textbf{0.336} & \textbf{0.424} & 0.156 & 0.183 \\
                & & \textbf{log}  & 0.329 & \textbf{0.417} & \textbf{-0.080} & \textbf{-0.096} & \textbf{0.298} & \textbf{0.363} & \textbf{0.263} & \textbf{0.322} & \textbf{0.336} & \textbf{0.424} & 0.156 & 0.183 \\
                & & \textbf{exp}  & 0.329 & \textbf{0.417} & \textbf{-0.080} & \textbf{-0.096} & \textbf{0.298} & \textbf{0.363} & \textbf{0.263} & \textbf{0.322} & \textbf{0.336} & \textbf{0.424} & 0.156 & 0.183 \\
                & & \textbf{sqrt} & 0.329 & \textbf{0.417} & \textbf{-0.080} & \textbf{-0.096} & \textbf{0.298} & \textbf{0.363} & \textbf{0.263} & \textbf{0.322} & \textbf{0.336} & \textbf{0.424} & 0.156 & 0.183 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{max}}
                  & \textbf{id}   & 0.038 & 0.046 & -0.165 & -0.210 & 0.239 & 0.291 & 0.105 & 0.135 & 0.072 & 0.090 & \textbf{0.159} & \textbf{0.201} \\
                & & \textbf{log}  & 0.038 & 0.046 & -0.165 & -0.210 & 0.239 & 0.291 & 0.105 & 0.135 & 0.072 & 0.090 & \textbf{0.159} & \textbf{0.201} \\
                & & \textbf{exp}  & 0.038 & 0.046 & -0.165 & -0.210 & 0.239 & 0.291 & 0.105 & 0.135 & 0.072 & 0.090 & \textbf{0.159} & \textbf{0.201} \\
                & & \textbf{sqrt} & 0.038 & 0.046 & -0.165 & -0.210 & 0.239 & 0.291 & 0.105 & 0.135 & 0.072 & 0.090 & \textbf{0.159} & \textbf{0.201} \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{sum}}
                  & \textbf{id}   & \textbf{0.330} & 0.411 & -0.098 & -0.114 & 0.273 & 0.342 & 0.132 & 0.166 & 0.305 & 0.389 & 0.052 & 0.059 \\
                & & \textbf{log}  & \textbf{0.330} & 0.411 & -0.098 & -0.114 & 0.273 & 0.342 & 0.132 & 0.166 & 0.305 & 0.389 & 0.052 & 0.059 \\
                & & \textbf{exp}  & \textbf{0.330} & 0.411 & -0.098 & -0.114 & 0.273 & 0.342 & 0.132 & 0.166 & 0.305 & 0.389 & 0.052 & 0.059 \\
                & & \textbf{sqrt} & \textbf{0.330} & 0.411 & -0.098 & -0.114 & 0.273 & 0.342 & 0.132 & 0.166 & 0.305 & 0.389 & 0.052 & 0.059 \\
        \midrule

        \multirow{16}{*}{\rotatebox{90}{\makecell{\textbf{Pairwise Preferences} \\ \textbf{flan-t5-small}}}}
        & \multirow{4}{*}{\textbf{mean}}
                  & \textbf{id}   & 0.063 & 0.073 & -0.192 & -0.232 & 0.306 & 0.375 & 0.071 & 0.090 & 0.316 & 0.387 & \textbf{0.212} & \textbf{0.269} \\
                & & \textbf{log}  & 0.063 & 0.073 & -0.192 & -0.232 & 0.306 & 0.375 & 0.071 & 0.090 & 0.316 & 0.387 & \textbf{0.212} & \textbf{0.269} \\
                & & \textbf{exp}  & 0.063 & 0.073 & -0.192 & -0.232 & 0.306 & 0.375 & 0.071 & 0.090 & 0.316 & 0.387 & \textbf{0.212} & \textbf{0.269} \\
                & & \textbf{sqrt} & 0.063 & 0.073 & -0.192 & -0.232 & 0.306 & 0.375 & 0.071 & 0.090 & 0.316 & 0.387 & \textbf{0.212} & \textbf{0.269} \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{min}}
                  & \textbf{id}   & \textbf{0.142} & \textbf{0.176} & \textbf{-0.167} & \textbf{-0.205} & \textbf{0.376} & \textbf{0.468} & \textbf{0.169} & \textbf{0.219} & \textbf{0.338} & \textbf{0.407} & 0.161 & 0.210 \\
                & & \textbf{log}  & \textbf{0.142} & \textbf{0.176} & \textbf{-0.167} & \textbf{-0.205} & \textbf{0.376} & \textbf{0.468} & \textbf{0.169} & \textbf{0.219} & \textbf{0.338} & \textbf{0.407} & 0.161 & 0.210 \\
                & & \textbf{exp}  & \textbf{0.142} & \textbf{0.176} & \textbf{-0.167} & \textbf{-0.205} & \textbf{0.376} & \textbf{0.468} & \textbf{0.169} & \textbf{0.219} & \textbf{0.338} & \textbf{0.407} & 0.161 & 0.210 \\
                & & \textbf{sqrt} & \textbf{0.142} & \textbf{0.176} & \textbf{-0.167} & \textbf{-0.205} & \textbf{0.376} & \textbf{0.468} & \textbf{0.169} & \textbf{0.219} & \textbf{0.338} & \textbf{0.407} & 0.161 & 0.210 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{max}}
                  & \textbf{id}   & -0.049 & -0.063 & -0.187 & -0.220 & 0.146 & 0.180 & 0.009 & 0.012 & 0.177 & 0.217 & 0.125 & 0.162 \\
                & & \textbf{log}  & -0.049 & -0.063 & -0.187 & -0.220 & 0.146 & 0.180 & 0.009 & 0.012 & 0.177 & 0.217 & 0.125 & 0.162 \\
                & & \textbf{exp}  & -0.049 & -0.063 & -0.187 & -0.220 & 0.146 & 0.180 & 0.009 & 0.012 & 0.177 & 0.217 & 0.125 & 0.162 \\
                & & \textbf{sqrt} & -0.049 & -0.063 & -0.187 & -0.220 & 0.146 & 0.180 & 0.009 & 0.012 & 0.177 & 0.217 & 0.125 & 0.162 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{sum}}
                  & \textbf{id}   & 0.063 & 0.073 & -0.192 & -0.232 & 0.306 & 0.375 & 0.071 & 0.090 & 0.316 & 0.387 & \textbf{0.212} & \textbf{0.269} \\
                & & \textbf{log}  & 0.063 & 0.073 & -0.192 & -0.232 & 0.306 & 0.375 & 0.071 & 0.090 & 0.316 & 0.387 & \textbf{0.212} & \textbf{0.269} \\
                & & \textbf{exp}  & 0.063 & 0.073 & -0.192 & -0.232 & 0.306 & 0.375 & 0.071 & 0.090 & 0.316 & 0.387 & \textbf{0.212} & \textbf{0.269} \\
                & & \textbf{sqrt} & 0.063 & 0.073 & -0.192 & -0.232 & 0.306 & 0.375 & 0.071 & 0.090 & 0.316 & 0.387 & \textbf{0.212} & \textbf{0.269} \\
        \midrule

        \multirow{16}{*}{\rotatebox{90}{\makecell{\textbf{Pairwise Preferences} \\ \textbf{t5-small}}}}
        & \multirow{4}{*}{\textbf{mean}}
                  & \textbf{id}   & \textbf{0.074} & \textbf{0.105} & -0.001 & 0.001 & 0.094 & 0.105 & -0.052 & -0.067 & -0.026 & -0.037 & 0.100 & 0.130 \\
                & & \textbf{log}  & \textbf{0.074} & \textbf{0.105} & -0.001 & 0.001 & 0.094 & 0.105 & -0.052 & -0.067 & -0.026 & -0.037 & 0.100 & 0.130 \\
                & & \textbf{exp}  & \textbf{0.074} & \textbf{0.105} & -0.001 & 0.001 & 0.094 & 0.105 & -0.052 & -0.067 & -0.026 & -0.037 & 0.100 & 0.130 \\
                & & \textbf{sqrt} & \textbf{0.074} & \textbf{0.105} & -0.001 & 0.001 & 0.094 & 0.105 & -0.052 & -0.067 & -0.026 & -0.037 & 0.100 & 0.130 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{min}}
                  & \textbf{id}   & 0.059 & 0.073 & \textbf{0.132} & \textbf{0.162} & \textbf{0.117} & \textbf{0.134} & -0.032 & -0.038 & -0.029 & -0.029 & 0.052 & 0.067 \\
                & & \textbf{log}  & 0.059 & 0.073 & \textbf{0.132} & \textbf{0.162} & \textbf{0.117} & \textbf{0.134} & -0.032 & -0.038 & -0.029 & -0.029 & 0.052 & 0.067 \\
                & & \textbf{exp}  & 0.059 & 0.073 & \textbf{0.132} & \textbf{0.162} & \textbf{0.117} & \textbf{0.134} & -0.032 & -0.038 & -0.029 & -0.029 & 0.052 & 0.067 \\
                & & \textbf{sqrt} & 0.059 & 0.073 & \textbf{0.132} & \textbf{0.162} & \textbf{0.117} & \textbf{0.134} & -0.032 & -0.038 & -0.029 & -0.029 & 0.052 & 0.067 \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{max}}
                  & \textbf{id}   & 0.063 & 0.099 & 0.025 & 0.035 & 0.107 & 0.129 & \textbf{0.049} & \textbf{0.054} & \textbf{0.069} & \textbf{0.083} & \textbf{0.109} & \textbf{0.139} \\
                & & \textbf{log}  & 0.063 & 0.099 & 0.025 & 0.035 & 0.107 & 0.129 & \textbf{0.049} & \textbf{0.054} & \textbf{0.069} & \textbf{0.083} & \textbf{0.109} & \textbf{0.139} \\
                & & \textbf{exp}  & 0.063 & 0.099 & 0.025 & 0.035 & 0.107 & 0.129 & \textbf{0.049} & \textbf{0.054} & \textbf{0.069} & \textbf{0.083} & \textbf{0.109} & \textbf{0.139} \\
                & & \textbf{sqrt} & 0.063 & 0.099 & 0.025 & 0.035 & 0.107 & 0.129 & \textbf{0.049} & \textbf{0.054} & \textbf{0.069} & \textbf{0.083} & \textbf{0.109} & \textbf{0.139} \\
        \cmidrule{2-15}
        & \multirow{4}{*}{\textbf{sum}}
                  & \textbf{id}   & \textbf{0.074} & \textbf{0.105} & -0.001 & 0.001 & 0.094 & 0.105 & -0.052 & -0.067 & -0.026 & -0.037 & 0.100 & 0.130 \\
                & & \textbf{log}  & \textbf{0.074} & \textbf{0.105} & -0.001 & 0.001 & 0.094 & 0.105 & -0.052 & -0.067 & -0.026 & -0.037 & 0.100 & 0.130 \\
                & & \textbf{exp}  & \textbf{0.074} & \textbf{0.105} & -0.001 & 0.001 & 0.094 & 0.105 & -0.052 & -0.067 & -0.026 & -0.037 & 0.100 & 0.130 \\
                & & \textbf{sqrt} & \textbf{0.074} & \textbf{0.105} & -0.001 & 0.001 & 0.094 & 0.105 & -0.052 & -0.067 & -0.026 & -0.037 & 0.100 & 0.130 \\
        \midrule

        \multirow{5}{*}{\rotatebox{90}{\makecell{\textbf{Pointwise} \\ \textbf{Preferences}}}}
            & & & & & & & & & & & & & & \\
            & \multicolumn{2}{c}{\textbf{flan-t5-base}}  & 0.131 & 0.174 & -0.046 & -0.058 & -0.017 & -0.020 & 0.033 & 0.038 & 0.077 & 0.091 & 0.207 & 0.255 \\
            & \multicolumn{2}{c}{\textbf{flan-t5-small}} & 0.101 & 0.131 & -0.043 & -0.056 & 0.021 & 0.021 & 0.110 & 0.139 & 0.088 & 0.103 & 0.101 & 0.134 \\
            & \multicolumn{2}{c}{\textbf{t5-small}}      & 0.183 & 0.227 & 0.134 & 0.166 & 0.063 & 0.074 & 0.095 & 0.119 & 0.119 & 0.152 & 0.122 & 0.165 \\
            & & & & & & & & & & & & & & \\
        \bottomrule 
    \end{tabular}}
    \renewcommand{\arraystretch}{1.0}
\end{table}
\begin{figure}[t!]
    \centering
    \begin{tabular}{ccc}
        \textbf{Pairwise Preferences} & \hspace{2cm} & \textbf{Pointwise Preferences} \\
    \end{tabular}
    \footnotesize
    % First row
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pairwise_cw22_score_distribution_flan-t5-base.pdf}
        \label{fig:pairwise_flan-t5-base}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pointwise_cw22_score_distribution_flan-t5-base.pdf}
        \label{fig:pointwise_flan-t5-base}
    \end{subfigure}

    \vspace{-0.5cm}
    \textbf{(a)} Relevance scores generated using \texttt{flan-t5-base}.
    \vspace{0.5cm}

    % Second row
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pairwise_cw22_score_distribution_flan-t5-small.pdf}
        \label{fig:pairwise_flan-t5-small}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pointwise_cw22_score_distribution_flan-t5-small.pdf}
        \label{fig:pointwise_flan-t5-small}
    \end{subfigure}

    \vspace{-0.5cm}
    \textbf{(b)} Relevance scores generated using \texttt{flan-t5-small}.
    \vspace{0.5cm}

    % Third row
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pairwise_cw22_score_distribution_t5-small.pdf}
        \label{fig:pairwise_t5-small}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/seaborn/pointwise_cw22_score_distribution_t5-small.pdf}
        \label{fig:pointwise_t5-small}
    \end{subfigure}

    \vspace{-0.5cm}
    \textbf{(c)} Relevance scores generated using \texttt{t5-small}.
    \vspace{0.5cm}

    \caption{Distributions of the inferred relevance scores for the pooled documents transferred to \texttt{ClueWeb22/b} across all six retrieval tasks, using the pairwise and pointwise approaches with different versions of the \texttt{T5} model.}
    \label{fig:score-distributions-cw22}
\end{figure}
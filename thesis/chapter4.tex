\chapter{Evaluation}\label{evaluation}

This chapter will evaluate the methods used for document segmentation and relevance assignment.

\section{Inter-annotator Agreement}\label{inter-annotator-agreement}

In order to guarantee the validity of the inferred relevance assignments and the suitability of the dataset for training purposes, it is essential to evaluate the accuracy of the assigned labels, particularly in the context of natural language processing applications. This evaluation is typically performed by calculating the Inter-Annotator Agreement (IAA) between the annotators who labelled the dataset.

The Inter-Annotator Agreement is a statistical measure that quantifies the consistency between the annotations provided by multiple annotators in a collaborative annotation project. It quantifies the level of agreement or disagreement between annotators when labelling the same dataset, thereby providing insight into the objectivity and quality of the annotations. A high IAA indicates the presence of reliable and transparent annotation guidelines, whereas a low IAA may be indicative of task ambiguity or inconsistencies in annotator interpretation.

There are a variety of methods for calculating IAA, each with its strengths and weaknesses. In this thesis, I will utilise the Cohen's Kappa coefficient, which is a statistical measure that can be used to assess the degree of agreement between annotators when categorising data. In this research, each document query pair will be assigned a relevance score of 0, 1, or 2. A score of 0 indicates that the document is not relevant to the query, a score of 1 indicates that the document is somewhat relevant, and a score of 2 indicates that the document is highly relevant.

\begin{itemize}
    \item Score of 0, 1 or 2 correct?
    \item Formular
    \item IAA sources
    \begin{itemize}
        \item \href{https://medium.com/@prasanNH/inter-annotator-agreement-in-natural-language-processing-f65685a22816}{Medium}
        \item \href{https://publications.goettingen-research-online.de/bitstream/2/111141/1/DWP44_Konle-IAA-Metriken.pdf}{Messverfahren zum Inter-annotator-agreement (IAA)}
        \item \href{https://fortext.net/ueber-fortext/glossar/inter-annotator-agreement-iaa}{fortext}
    \end{itemize}
    \item cohen kappa sources
    \begin{itemize}
        \item \href{https://scikit-learn.org/dev/modules/generated/sklearn.metrics.cohen_kappa_score.html}{scikit-learn}
        \item \href{https://surge-ai.medium.com/inter-annotator-agreement-an-introduction-to-cohens-kappa-statistic-dcc15ffa5ac4}{Medium}
    \end{itemize}
\end{itemize}

\section{Rank Correlation}\label{rank-correlation}  

To assess the relationship between the relevance labels and the calculated scores, a correlation analysis was conducted. Relevance labels represent an ordinal scale with values of 0, 1, and 2, denoting increasing levels of relevance, while calculated scores range from -1 to 1, representing the model's prediction for each document's relevance with respect to the query. Given the distinct nature of these data types, it was necessary to select a correlation measure that could accommodate both ordinal and continuous data without assuming a linear relationship.

\subsection{Spearman's Rank Correlation Coefficient}\label{spearmans-rank-correlation-coefficient}

Among the common correlation measures—Pearson's, Spearman's, and Kendall's—the Spearman's rank correlation coefficient was selected as the most appropriate measure for this analysis. Spearman's correlation is particularly suited to this context for several reasons:

\begin{enumerate}
    \item Ordinal Nature of Relevance Labels: The relevance labels are ordinal, meaning they indicate an ordered relationship (0 < 1 < 2), but the intervals between values may not represent equal differences in relevance. Spearman's correlation is designed for ranked or ordinal data, making it ideal for assessing relationships where the exact distance between values is less meaningful than the order.
    \item Monotonic Relationship Requirement: Spearman's correlation assesses whether there is a monotonic relationship between two variables rather than a strict linear relationship. This is important given that the relevance labels and calculated scores may not vary linearly but are expected to follow a general trend (e.g., higher relevance labels should be associated with higher calculated scores).
    \item Robustness to Outliers and Non-Normality: Unlike Pearson's correlation, Spearman's correlation does not assume normally distributed data or homoscedasticity, which is suitable given the categorical nature of relevance labels and potential non-normal distribution of the calculated scores.
\end{enumerate}

\begin{itemize}
    \item +1: perfect monotonic agreement between rankings
    \item above \~ 0.6: strong monotonic relationship
    \item 0: no monotonic relationship
    \item -1: perfect monotonic decrease
\end{itemize}

\subsection{Kendall's Tau Rank Correlation Coefficient}\label{kendalls-tau-rank-correlation-coefficient}

Kendall's Tau measures the ordinal association between two variables by comparing the number of concordant and discordant pairs of data. It's especially useful for small datasets and ordinal data, offering a more nuanced view of monotonic relationships. The Kendall coefficient ranges from -1 to +1, with higher absolute values indicating stronger associations.

\begin{itemize}
    \item +1: perfect agreement between rankings
    \item above \~ 0.5: strong positive association
    \item 0: no association between rankings
    \item -1: perfect disagreement between rankings
\end{itemize}

\subsection{Pearson's Correlation Coefficient}\label{pearsons-correlation-coefficient}

Pearson's Correlation measures the linear relationship between two continuous variables. It assumes that the data is normally distributed and is sensitive to outliers. The result, called the Pearson correlation coefficient (r), ranges from -1 (perfect negative correlation) to +1 (perfect positive correlation), with 0 indicating no linear correlation.

\begin{itemize}
    \item +1: perfect positive linear relationship
    \item above \~ 0.7: strong positive linear relationship
    \item 0: no linear relationship
    \item -1: perfect negative linear relationship
\end{itemize}

\subsection{Unit testing of correlation measures}\label{unit-testing-of-correlation-measures}

% Define the inputs
The following evaluations involve six inputs labeled as A, B, C, D, E, and F. 
The input A represents the ground truth with values \([0.2, 0.7, 0.8, 0.5, 0.3]\). 
The other inputs are designed to test the rank correlation methods against A:
\begin{itemize}
    \item \(B = [0.2, 0.7, 0.8, 0.5, 0.3]\): Expected high correlation with A.
    \item \(C = [0, 2, 2, 1, 0]\): Expected high correlation with A.
    \item \(D = [0, 1, 2, 1, 0]\): Expected high correlation with A.
    \item \(E = [0, 0, 1, 0, 0]\): Expected moderate correlation with A.
    \item \(F = [0, 0, 1, 1, 0]\): Expected lower correlation with A.
\end{itemize}

% Table
\begin{table}[h!]
    \centering
    \caption{Evaluation of Rank Correlation Methods with Unit Test Cases}
    \begin{tabular}{@{}llcc@{}}
        \toprule
        \textbf{Rank Correlation Method} & \textbf{Input (A and ...)} & \textbf{Standard Implementation} & \textbf{Greedy Implementation} \\ \midrule
        Kendall                        & A and B                   & 0.8944                          & 1.0                            \\
        Kendall                        & A and C                   & 0.8944                          & 1.0                            \\
        Kendall                        & A and D                   & 0.6325                          & 1.0                            \\
        Kendall                        & A and E                   & 0.5164                          & 0.6667                         \\ \midrule
        Pearson                        & A and B                   & 0.9806                          & 1.0                            \\
        Pearson                        & A and C                   & 0.9376                          & 1.0                            \\
        Pearson                        & A and D                   & 0.6578                          & 1.0                            \\
        Pearson                        & A and E                   & 0.5371                          & 0.6667                         \\ \midrule
        Spearman                       & A and B                   & 0.9487                          & 1.0                            \\
        Spearman                       & A and C                   & 0.9487                          & 1.0                            \\
        Spearman                       & A and D                   & 0.7071                          & 1.0                            \\
        Spearman                       & A and E                   & 0.5774                          & 0.6667                         \\ \bottomrule
    \end{tabular}
\end{table}

\end{document}
\chapter{Related Work}\label{related-work}

The idea of transferring relevance judgments from one dataset to another has been explored in multiple studies. \citet{froebe:2021} investigated the issue of near-duplicate documents within widely-used web crawls such as \texttt{ClueWeb} and \texttt{Common Crawl}. The authors proposed a deduplication approach to address this issue and, as an extension, explored the potential of transferring relevance labels between near-duplicate documents. However, their study also emphasized that newly collected judgments remain necessary for evaluating retrieval systems on newer datasets. The \texttt{MS MARCO} dataset is an example of ineffective relevance judgment transfer. \citet{froebe:2022} analyzed  why retrieval models trained on \texttt{MS MARCO Version 1} performed better than those trained on \texttt{MS MARCO Version 2}. The \texttt{MS MARCO} crawl is a widely used training and test dataset in information retrieval, available both as a document corpus and as a passage corpus. In version one, the passage dataset was available and judged before the document dataset. To take advantage of this, relevance judgments were directly transferred from the passage dataset to the document dataset by assigning the same relevance label to any document that had the same URL as the judged passage. A similar process was applied to enrich version two, where documents were assigned the same relevance label as their corresponding documents from version one if they had the same URL. The issue with this approach was that the document corpus in version one was crawled one year after the passage corpus, during which time the content of URLs remained largely unchanged. However, the document corpus in version two was crawled four years after the passage corpus, leading to many content changes. This dissimilarity resulted in inaccurate relevance transfers. This example highlights the importance of considering document content when transferring relevance judgments between datasets. To address this challenge, the transfer pipeline in this thesis employs a pairwise preference approach to compare old and new documents, ensuring a more reliable transfer of relevance information.
%An example for not transferring relevance judgments well is the \texttt{MS MARCO} dataset. \citet{froebe:2022} researched why the performance of retrieval models tranined on \texttt{MS MARCO Version 1} were more effective then trained on \texttt{MS MARCO Version 2}. \texttt{MS MARCO} is a widely used trainings and test dataset in information retrieval which has the advantage that it is available as document corpus and as passage corpus. The \texttt{MS MARCO} passage dataset of version 1 was judged before the document dataset. To leverage this, the relevance judgments were directly transferred from the passage dataset to the document dataset. This was done by assigning documents with the same URL as a judged passage the same relevance label. To enrich version 2 of \texttt{MS MARCO} with relevance judgments the relevance judgments were transferred identically, assigning the same relevance label to documents in version 2 with the same URL as a judged documents in version 1. The problem with this approach was, that the crawled document corpus of version 1 was done one year after the passage corpus. In that time, the content of the URLs did not change or not much. However, the crawl of version 2 of \texttt{MS MARCO} document corpus was done four years after the passage corpus. In that time, the content of the URLs can changed significantly. This shows that transferring relevance judgments between different datasets should rely also on the content of the documents. By employing a pairwise preference approach between the old and new documents, the relevance transfer pipeline in this thesis aims to address this challenge.
\\\\
A more automated approach to infer relevance judgments was introduced by \citet{macavaney:2023}, who proposed One-Shot Labelers ($\mathbb{1}$SL) to predict relevance for unjudged passages using nearest neighbor searches and various prompting strategies. Their goal was to examine the potential of large language models to fill \glqq holes\grqq{} (i.e., unjudged documents) in relevance assessments for information retrieval evaluations. Their results demonstrated that instruction-tuned models, such as \texttt{FlanT5} \citep{chung:2022}, can provide reliable relevance estimations. However, their study focused only on passages and left document level inference as future work. This thesis addresses that gap by employing a pairwise inference approach with \texttt{FlanT5} to infer relevance labels at the document level.
\\\\
Re-ranking methods play a crucial role in improving document ranking effectiveness. Traditional approaches include pointwise, pairwise, and listwise ranking, each with a trade-off between efficiency and effectiveness. \citet{pradeep:2021} introduced a multi-stage ranking framework that combines document expansion, pointwise ranking, and pairwise re-ranking to enhance retrieval performance. Their findings showed that pairwise ranking models, such as \texttt{DuoT5}, significantly improve ranking quality, even in zero-shot scenarios. Inspired by this, the relevance transfer pipeline in this thesis leverages pairwise inference to determine the relevance of documents in the target corpus.
\\\\
One of the key challenges in the relevance transfer pipeline is the selection of candidates for relevance transfer. The selection is a critical step, as it determines which documents should be considered for relevance inference. \mbox{\citet{macavaney:2022}} highlighted a key limitation in re-ranking pipelines: \glqq re-ranking pipelines are limited by the recall of the initial candidate pool and therefore documents that are not identified as candidates for re-ranking by the initial retrieval function cannot be identified \grqq{}. To address this, they proposed a Graph-based Adaptive Re-ranking (\texttt{GAR}), which expands the candidate pool beyond the initial retrieval set by leveraging the clustering hypothesis \citep{jardine:1971}. This hypothesis suggests that closely related documents are often relevant to the same queries. Inspired by this principle, this thesis explores various nearest-neighbor strategies to perform the selection of documents for relevance transfer.
\\\\\\\\\\\\
Processing pairwise preferences can be computationally expensive, particularly when dealing with large document collections, since it requires evaluating $k^2-k$ preferences for $k$ documents. To address this challenge, \mbox{\citet{gienapp:2022}} investigated whether sampling from all possible preference pairs could improve the efficiency of pairwise re-ranking models without sacrificing effectiveness. Their findings showed that re-ranking effectiveness could be maintained with only one-third of the usual comparisons, with only a minor performance drop when further reducing comparisons. Based on that, the number of pairwise comparisons in the relevance transfer pipeline must not be exhaustive to maintain efficiency. Similarly, \citet{zhuang:2024} introduced \glqq Setwise prompting \grqq{}, a novel zero-shot document ranking approach designed to reduce the number of required inferences while balancing efficiency and effectiveness. Their results demonstrated the advantages of Setwise prompting over traditional pairwise methods, suggesting that it could serve as a more fine-grained alternative for pairwise inference in relevance transfer.
\\\\
Overall, these studies provide essential insights into relevance transfer, candidate retrieval, and efficiency optimizations, forming the foundation for the relevance transfer pipeline developed in this thesis.

% \cite{froebe:2021} addresses the issue of near-duplicate documents in widely-used web crawls, such as ClueWeb09, ClueWeb12, and Common Crawl, which can have a distorting effect on the reliability and validity of information retrieval experiments. The paper proposes the possibility of transferring relevance judgments from older crawls to newer ones, which is the area I aim to investigate further in this thesis. While the authors found that some relevance judgments could be reused, they concluded that new judgments are still necessary for shared tasks on newer datasets. This research will build on this work by focusing on improving the effectiveness of transferring these judgments across different crawls.
% \\\\
% The objective of the \citet{gienapp:2022} project was to enhance the efficiency of pairwise re-ranking models in information retrieval while maintaining effectiveness. However, these models typically face challenges due to high inference overhead, which is caused by the quadratic number of document comparisons required. To overcome this issue, three sampling methods and five preference aggregation methods were evaluated. The results demonstrated that re-ranking effectiveness could be achieved with only one-third of the usual comparisons, and even fewer comparisons could yield only a slight decrease in effectiveness.
% \\\\
% In the field of information retrieval, \citet{mackenzie:2021} evaluated the impact of considering multiple relevant passages per query on the performance assessment of retrieval systems using the MSMARCO passage dataset. The objective was to ascertain the impact of the addition of plausible but previously unjudged relevant passages on system scores and comparative rankings. The results demonstrated that the addition of plausible, relevant passages can result in considerable variation in individual run scores, while the overall system rankings remain relatively stable. These results provide support for the methodologies used in constructing the MSMARCO passage collection.
% \\\\
% \citet{pradeep:2021} examined the potential of a multi-stage ranking approach for text retrieval. The \\glqq Expando\glqq  component employed document expansion using a sequence-to-sequence model to enhance the representation of keywords in texts. The \glqq Mono\glqq  component applied a pointwise ranking model, while the \glqq Duo\glqq  component utilised a pairwise model for final reranking. The design was validated on datasets such as MS MARCO and TREC-COVID, achieving near state-of-the-art performance and demonstrating effectiveness even in zero-shot scenarios. Building on these findings, the present research will examine the use of Large Language Models to transfer relevance judgments across different datasets.
% \\\\
% The most commonly employed methods for zero-shot ranking of documents utilising Large Language Models are pointwise, pairwise and listwise. The research conducted by \citet{zhuang:2024} proposed a new approach to zero-shot document ranking, termed \glqq Setwise prompting\glqq . The objective of this method is to enhance the existing approaches by reducing the number of inferences during ranking, thereby achieving a more balanced trade-off between efficiency and effectiveness. The study presents a comprehensive and systematic comparison of these methods, thereby demonstrating the advantages of Setwise in the context of zero-shot ranking.
% \\\\
% \cite{macavaney:2023} examines the potential of Large Language Models to fill \glqq holes\glqq  in relevance assessments for information retrieval evaluations, particularly in cases where only one relevant document per query is available. It proposes One-Shot Labelers ($\mathbb{1}$SL) that utilize techniques like nearest neighbor searches, supervised models, and prompting to predict relevance for unjudged documents. While these methods have been shown to have limitations in identifying all relevant documents, they achieve a high correlation with rankings from full assessments using recall-agnostic measures.The study suggests that $\mathbb{1}$SL can complement human annotations, improving less time and expanding evaluation scope, though not fully replacing manual relevance judgments. The instruction-tuned model FlanT5 (\citet{chung:2022}) turned out to be a very reliable method to infer relevance labels for the unjudged documents. Therefore, this model is also used in this work to take a query, a relevant document for this query and a document that is not judged. The model infers a value for the unjudged document in terms of relevance to the query.
